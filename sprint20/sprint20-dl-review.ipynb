{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sprint20課題 深層学習復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深層学習とは何か\n",
    "深層学習（ディープラーニング）は、機械学習の手法の１つであり、連続する層（layer）の学習に重点が置かれる。すなわち、深層学習は一般的には深い層から構成されるネットワークを指す。それらの層が深くなるほど、表現の重要性は増していく。深層学習の基本的な原理は、損失の勾配をフィードバックとして使用することで、重みの値を少しずつ調整していくというものである。重みの調整は、現在のサンプルにおいて重みの勾配が低くなる方向に向かって行われる。バックプロパゲーション（誤差逆伝播法）で、誤差が小さくなるように全体の結合重みを調整している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深層学習では何ができるのか\n",
    "深層学習は、従来の機械学習では難しいとされてきた、以下の分野で大きなブレークスルーを果たしてきた。   \n",
    "・人に近いレベルの画像分類   \n",
    "・人に近いレベルの音声認識    \n",
    "・人に近いレベルの手書き文字認識   \n",
    "・機械翻訳の改善      \n",
    "・テキスト音声変換の改善   \n",
    "・ターゲティング広告の改善 など   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 他の機械学習手法よりも深層学習が適しているデータや状況は何か\n",
    "・従来の機械学習では、人が特徴量を設計する。例えば、画像を分類する際に、色に着目するのか、エッジに着目するのか、またその範囲などをどうするのかなどを予め人が決めて、抽出した特徴量を用いて機械学習を行っている。   \n",
    "・一方、深層学習は、そのような情報に着目すればよいのかを大量のデータから決めてくれる。そのため、人だとなかなか考えつかないような色やエッジを組み合わせたような特徴量も獲得できる。たとえば、画像認識ではCNNを使えば、大量のデータを使っての学習を行える。また、深層学習の手法のみならず、ImageNetに代表される大規模なデータセットの整備や、高速な数値演算を可能にするGPUなどが低コストで手に入るようになったことも、深層学習の大幅な性能改善に貢献している。これにより、難しい認識問題でもトップレベルの性能を出すことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深層学習の層の組み方はどのように決定するべきか\n",
    "・深層学習のニューラルネットワークは、一般的に入力層、中間層（隠れ層）、出力層の3層構造となっており、各層は結合重みでつながっている。深層学習でよく利用される畳み込みニューラルネットワーク（CNN:Convolutional Neural Network）では、入力層（input layer）、畳み込み層（convolution layer）、プーリング層（pooling layer）、全結合層（fully connected layer）、出力層（output layer）から構成されている。畳み込み層とプーリング層は複数回繰り返して深い層を形成し、その後も結合層も同様に何層か続く構造となっている。  \n",
    "・2012年に画像のコンペ（ILSVRC）で畳み込みニューラルネットワークをもとにしたネットワークモデルが優勝して以降、様々なネットワークモデルが登場している。たとえば、層を深くすることを可能にしたVGGNetや、勾配消失しないようにスキップ接続を導入したResNetなどを利用することがよく行われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数はどのように選択すれば良いか\n",
    "・ニューラルネットワークでは活性化関数としてシグモイド関数が主に利用されるが、深層学習ではReLUが一般的によく利用される。ReLUは、入力が0を超えていれば、その入力をそのまま出力し、0以下ならば0を出力する。また、その微分値は0を超えていれば1となる。それ以外の場合は0となる関数。    \n",
    "・深層学習ではモデルのパラメータを学習するために、誤差伝搬法という方法を繰り返し用いる。モデルから出力されたデータと解答を照らし合わせて, 誤差を求めるが, その際、前回の誤差との差分に注目する。この時、誤差に微分値を使うが、ReLUを使うと、この微分は入力値が0を超えていれば１となり、誤差の微分を０にすることなく、結合重みを更新する。また、それ以外の場合は0となるため、結合重みは更新されない。    \n",
    "・また、ReLUを改良した活性化関数として、Leaky ReLU、PReLU、RReLUがあり、負の値を出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習曲線を確認して学習データと検証データの損失の差が大きかった場合にはまず何をすれば良いか\n",
    "学習曲線を確認して学習データと検証データの損失の差が大きかった場合は過学習に陥っていると考えられるため、まずは、正則化やドロップアウトを行うことが必要である。他には、畳み込み層のフィルタの数や層の数を増やして複雑さを増やすことも有効である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深層学習はデータ量がある程度必要だとされているがそれはなぜか\n",
    "・学習データの数が少なすぎると、深層学習の活性化関数（ReLUなど）自体の形状が単純であるため、新しいデータにうまく汎化するようにモデルを訓練することが困難になる。一方、学習データの数が多ければ、学習（訓練）データのバリエーションが増えていき、未知のデータに近づいていくことができる。   \n",
    "・深層学習はもちろん複雑なデータに対しては、とても効果を発揮する。しかし、どうしても計算コストがかかったり管理が大変だったりするのでデータ「分析＝深層学習」という構図で考えるのではなく、データセットやコストなど様々な面からみてできるだけシンプルなモデルで分析できるといい。     \n",
    "・極論予測を行うよりもEDAだけして、効率の悪いところを明らかにし、その数字を改善するための意識の改善やコストカットを測る方が効率のいい場合もある。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手元にあるデータが少ない場合はどうするか\n",
    "いまあるサンプルに変形を加えて枚数を増やすデータ拡張を行う。すなわち、データ拡張は、サンプルに対して、平行移動や回転移動を加えたり、鏡面反転に加えて、幾何学変化や濃淡・色の変動などをさせたりして、変化を加えた新たなサンプルを作成する方法である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
