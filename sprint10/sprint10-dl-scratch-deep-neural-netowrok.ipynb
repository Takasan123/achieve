{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIF_sprint10-dnn-scratch\n",
    "・スクラッチを通してニューラルネットワークの発展的内容を理解する      \n",
    "・コードをオブジェクト指向で書き換える経験をする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的関数（損失関数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 平均2乗和誤差\n",
    "class MeanSquardError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def mse(self, y, y_pred):\n",
    "        # y  :  正解ラベル\n",
    "        # y_pred  :  予測値\n",
    "        return np.mean((y - y_pred)**2)/2\n",
    "        \n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        return y_pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差\n",
    "class CrossEntropyError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cee(self, y_pred, y_train):\n",
    "        if y_pred.ndim == 1:\n",
    "            y_train = y_train.reshape(1, y_train.size)\n",
    "            y_pred = y_pred.reshape(1, y_pred.size)\n",
    "    \n",
    "        batch_size = y_pred.shape[0]\n",
    "        #ミニバッチ単位の損失\n",
    "        return -np.sum(y_train * np.log(y_pred + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "class ReLU:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #mask\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 活性化関数のクラス化\n",
    "# 活性化関数\n",
    "class Tanh:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.forward_A = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # backwardの時に必要なのでインスタンス変数として保持\n",
    "        self.forward_A = x\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        # ∂L/∂A2 = ∂L/∂z2 * (1 - tanh(A2)**2)\n",
    "        #(10, 200) アダマール積 (10, 200) => (10, 200)\n",
    "        delta = delta * (1 - np.tanh(self.forward_A)**2)\n",
    "        return  delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 出力層の活性化関数\n",
    "# ソフトマックス関数（MNIST用）\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.forward_A = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.forward_A = x\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            self.y_pred = y.T\n",
    "        else:\n",
    "            x = x - np.max(x)\n",
    "            self.y_pred = np.exp(x) / np.sum(np.exp(x))\n",
    "        return self.y_pred\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        delta = self.y_pred - delta\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差\n",
    "class CrossEntropyError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cee(self, y_pred, y_train):\n",
    "        if y_pred.ndim == 1:\n",
    "            y_train = y_train.reshape(1, y_train.size)\n",
    "            y_pred = y_pred.reshape(1, y_pred.size)\n",
    "    \n",
    "        batch_size = y_pred.shape[0]\n",
    "        #ミニバッチ単位の損失\n",
    "        return -np.sum(y_train * np.log(y_pred + 1e-7)) / batch_size\n",
    "    \n",
    "#     def backward(self, y, y_pred):\n",
    "#         return y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer（最適化手法のクラス化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD(Optimizer:最適化手法)のクラス化\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    lr  :  学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.001):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ある層の重みやバイアスの更新\n",
    "    --------------\n",
    "    layer   :   更新前の層のインスタンス\n",
    "    layer   :   更新後の層のインスタンス\n",
    "    \"\"\"\n",
    "    def update(self, layer):\n",
    "        \n",
    "        layer.W = layer.W - self.alpha * layer.dW / layer.dB.shape[0]\n",
    "        layer.B = layer.B - self.alpha * layer.dB.mean(axis=0)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__():\n",
    "        self.lr = 0.001\n",
    "        \n",
    "    def update():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer（初期化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 初期方法のクラス化\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma*np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    \n",
    "    Parameters\n",
    "    ---------------------\n",
    "    X　:　学習データ\n",
    "    y　:　正解値\n",
    "    batch_size : int\n",
    "    seed : int　　　NumPyの乱数のシード\n",
    "    \n",
    "    ---------------------\n",
    "    for文で呼び出すと以下の２つを返す\n",
    "    mini_X  : 学習データのミニバッチ \n",
    "    mini_y  : 正解値のミニバッチ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        \n",
    "        # バッチサイズを指定\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # ランダムに並べ換える\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # 48,000行、列？のベクトルが作成される\n",
    "        # 中身は1から48,000の整数値がランダムにセットされる\n",
    "        # なるほど、名前からしてそうだね。シャッフルインデックス。。\n",
    "        #print(\"testコード\", X.shape[0])\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        \n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        \n",
    "        # イテレーション数を計算する\n",
    "        # 48,000行/10 = 4,800回となる\n",
    "        # np.ceilは切り上げする関数\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len()が使われたときの処理\n",
    "        return self._stop\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "            \n",
    "        # for文で呼ばれた際のループごとの処理\n",
    "        if self._counter >= self._stop:\n",
    "                \n",
    "            # 最後まで進んだら終了\n",
    "            self._counter = 0 # カウンターをリセット\n",
    "            raise StopIteration()\n",
    "        \n",
    "        \n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "            \n",
    "        self._counter = self._counter + 1\n",
    "            \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全結合層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Full_Connection:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    n_nodes1 :  int\n",
    "    n_nodes2 :  int\n",
    "    initializer :  初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    # コンストラクタで重みやバイアスの初期化をする\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "\n",
    "        # 全結合層のコンストラクタに初期化方法のインスタンスを渡したい\n",
    "        # ノード数を保持する\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.sigma = initializer.sigma\n",
    "        \n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        # 重み・バイアスを初期化\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        #print(\"doudesuka\",n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "        #print(\"★★★★★★★\",self.B.shape)\n",
    "        \n",
    "        # 引数で受け取ったoptimizer（最適化クラス）をインスタンス変数として持つ\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # ここで初期化が必要。。\n",
    "        self.X = None\n",
    "        self.dB = None\n",
    "        self.dW = None\n",
    "        self.A = None\n",
    "        \n",
    "        # ドロップアウト用\n",
    "        self.mask = None\n",
    "        self.dropout_ratio = 0.5\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X  :  配列とシェイプ  （batch_size,  n_nodes1）\n",
    "        A  :  配列とシェイプ  （batch_size, n_nodes2）\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ドロップアウトする要素（False）としない要素（True）に分ける\n",
    "        \"\"\"\n",
    "        self.X = X.copy()\n",
    "        self.mask = np.random.rand(*self.X.shape) > self.dropout_ratio\n",
    "        self.X = self.X * self.mask\n",
    "        \n",
    "        # 入力Xをインスタンス変数として保持\n",
    "        self.X = X.copy()\n",
    "        \n",
    "        # 全結合はここで定義している\n",
    "        self.A = np.dot(self.X, self.W) + self.B\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \n",
    "        \"\"\"\n",
    "        dA  :  後ろから流れてきた勾配(batch_size, n_nodes2)\n",
    "        dZ  :   前に流す勾配　（batch_size, n_nodes1）\n",
    "        \"\"\"\n",
    "        # バイアスの勾配は、forward時に加算演算なので、\n",
    "        # 前から受け取った勾配をそもまま利用する\n",
    "        self.dB = dA\n",
    "        \n",
    "        # forward時に活性化関数を通ったXを利用する。Zに相当する\n",
    "        # 前から受け取ったもdAと、dot積をとる。ひっくり返すところの処理。乗算演算なので\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        \n",
    "        # 後ろに流す勾配\n",
    "        dZ = np.dot(dA, self.dW.T)\n",
    "        \n",
    "        # 引数として自身のインスタンスselfを渡すこともできる\n",
    "        # 層の重みの更新が可能である\n",
    "        # self.optimizer.update(self)だけでもOK\n",
    "        # 各層のbackwardの処理の中で、重みパラメータとバイアスを更新する処理です\n",
    "        # sprint9の時は最後にまとめて更新していたが、その層の重みとバイアスが求まったタイミングでリアルタイムで更新する\n",
    "        # また、ここでselfに代入しなくても動作するらしいです。\n",
    "        #print(\"fcで最適化する\")\n",
    "        self = self.optimizer.update(self)        \n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def dropout():        \n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DNNスクラッチ回帰\n",
    "class ScratchDeepNeuralNetworkRegressor:\n",
    "    def __init__(self, n_epochs, batch_size, lr, sigma, n_nodes1, n_nodes2, n_output):\n",
    "        self.n_epochs    =  n_epochs    # エポック数\n",
    "        self.batch_size  = batch_size   #バッチサイズ\n",
    "        self.lr                  =  lr                   # 学習率\n",
    "        self.sigma          =  sigma            # シグマ\n",
    "        self.n_nodes1   =  n_nodes1    # 1層目のノード数\n",
    "        self.n_nodes2   =  n_nodes2    # 2層目のノード数\n",
    "        self.n_output    = n_output      # 出力層のノード数\n",
    "        self.loss             =  None\n",
    "        self.val_loss     =  None\n",
    "        \n",
    "        self.loss_list = []\n",
    "        self.epoch_loss_list = []\n",
    "        \n",
    "        #　入力層の特徴量（今回は2種類）\n",
    "        # fitの中でインスタンス変数を定義するためには、ここで事前に定義する必要がある\n",
    "        self.n_features = None\n",
    "        \n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        \n",
    "        self.activation1 = None\n",
    "        self.activation2 = None\n",
    "        self.activation3 = None\n",
    "        \n",
    "        self.tomo = None\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "\n",
    "        #print(\"testコード0---\", y.shape)\n",
    "        #y = y[:, np.newaxis]\n",
    "        #print(\"testコード0---\", y.shape)        \n",
    "        \n",
    "        # ミニバッチ単位で処理する\n",
    "        train_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=0)\n",
    "        #test_batch = GetMiniBatch(X_val, y_val, batch_size=self.batch_size, seed=0)\n",
    "        \n",
    "        #print(\"testコード01---\", y_val.shape)\n",
    "        #y_val = y_val[:, np.newaxis]\n",
    "        #print(\"testコード01---\", y_val.shape)\n",
    "        \n",
    "        # 最適化関数（オプティマイザー）クラスをインスタンス化する\n",
    "        optimizer = SGD(self.lr)\n",
    "\n",
    "        # fitを呼び出す時に、ここで特徴量を把握すれば、コンストラクタでわざわざ定義しなくても良い\n",
    "        self.n_features = X.shape[1]        \n",
    "\n",
    "        # インスタンス化（開始） ########################################\n",
    "        # for文の外でインスタンス化しないと、いけないはず\n",
    "        # for文の中でインスタンス化してしまうと、ループで毎回初期化されて、\n",
    "        # backwardでforward時のA(=X・W+B)を使えなくなってしまうから\n",
    "        \n",
    "        # 1層目\n",
    "        # 全結合層クラスをインスタンス化する\n",
    "        # SimpleInitializerを引数の中でクラスを呼んでいる理由は、各層で異なる初期値を与えて上げることができるため\n",
    "        # optimizerを全結合で\n",
    "        self.fc1 = Full_Connection(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        # 活性化関数クラスをインスタンス化する\n",
    "        self.activation1 = Tanh()\n",
    "        #self.activation1 = ReLU()\n",
    "        \n",
    "        \n",
    "        # 2層目\n",
    "        # 全結合層クラスをインスタンス化する\n",
    "        self.fc2 = Full_Connection(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        # 活性化関数クラスをインスタンス化する\n",
    "        self.activation2 = Tanh()\n",
    "        #self.activation2 = ReLU()\n",
    "        \n",
    "        # 3層目\n",
    "        # 全結合層クラスをインスタンス化する\n",
    "        self.fc3 = Full_Connection(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        \n",
    "        # 平均二乗誤差クラスをインスタンス化する\n",
    "        self.activation3 = MeanSquardError()\n",
    "        # インスタンス化（終了） ########################################\n",
    "        \n",
    "                \n",
    "        # エポック単位のループ\n",
    "        for epoch in range(self.n_epochs):\n",
    "                       \n",
    "            # ミニバッチ単位のループ\n",
    "            #a = 0\n",
    "            for mini_X_train, mini_y_train in train_batch:      \n",
    "                #a = a +1\n",
    "                # フォワード ########################################\n",
    "                # 1層目\n",
    "                # 全結合層（A = X・W + B）\n",
    "                #print(\"testcode1\",mini_X_train.shape)\n",
    "                A1 = self.fc1.forward(mini_X_train)\n",
    "                # 活性化関数（Z = 活性化関数（A））\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "\n",
    "                # 2層目\n",
    "                # 全結合層（A = Z・W + B）\n",
    "                A2 = self.fc2.forward(Z1)\n",
    "                # 活性化関数（Z = 活性化関数（A））\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "\n",
    "                # 3層目\n",
    "                # 全結合層（A = Z・W + B）\n",
    "                A3 = self.fc3.forward(Z2)\n",
    "                # 活性化関数（Z = 活性化関数（A））　　・・・回帰の時は恒等関数\n",
    "                Z3 = A3\n",
    "                \n",
    "                self.loss = self.activation3.mse(mini_y_train, Z3)\n",
    "                self.loss_list.append(self.loss)\n",
    "\n",
    "                # バックワード ########################################\n",
    "                # 3層目\n",
    "                # 平均2乗誤差のバックワード\n",
    "                dA3 = self.activation3.backward(mini_y_train, Z3)\n",
    "                # 全結合層のバックワード\n",
    "                dZ2 = self.fc3.backward(dA3)\n",
    "\n",
    "                # 2層目\n",
    "                # 活性化関数のバックワード\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                # 全結合層のバックワード\n",
    "                dZ1 = self.fc2.backward(dA2)\n",
    "\n",
    "                # 1層目\n",
    "                # 活性化関数のバックワード\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                # 全結合層のバックワード\n",
    "                dZ0 = self.fc1.backward(dA1)\n",
    "            \n",
    "            #print(a)\n",
    "            self.epoch_loss_list.append(self.loss)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # 1層目  ############################################\n",
    "        A1 = self.fc1.forward(X)\n",
    "        # この時点ではOK\n",
    "        #print(A1.shape) # (292, 500)\n",
    "        #print(A1) # バラバラな値である\n",
    "        self.tomo = A1\n",
    "        \n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        #print(Z1.shape) # (292, 500)\n",
    "        #print(Z1)\n",
    "\n",
    "        # 2層目  ############################################\n",
    "        A2 = self.fc2.forward(Z1)\n",
    "        #print(A2.shape)\n",
    "        #print(A2)\n",
    "        \n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        #print(\"Z2.shape : \",Z2.shape)\n",
    "        #print(Z2)\n",
    "        #print(np.unique(Z2))\n",
    "\n",
    "        # 3層目  ############################################\n",
    "        A3 = self.fc3.forward(Z2)\n",
    "        #print(A3.shape)\n",
    "        Z3 = A3\n",
    "        \n",
    "        return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 2) (292, 2) (1168, 1) (292, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/Users/andoutakaaki/DIC_study/sprint10/train.csv\")\n",
    "X = data[[\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = data[\"SalePrice\"].values\n",
    "\n",
    "#print(X.shape)\n",
    "y = y[:, np.newaxis]\n",
    "# 正規化\n",
    "X = np.log(X)\n",
    "y = np.log(y)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インスタンス生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdnnr1 = ScratchDeepNeuralNetworkRegressor(n_epochs=1000, batch_size=50, lr=0.0001,sigma=0.01, n_nodes1=3, n_nodes2=2, n_output=1)\n",
    "sdnnr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDkvs9bRwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# エポック毎の損失値\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(sdnnr1.epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/CIiBUbBLyJSYP4/BZKxQb+iMccAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ミニバッチ毎の損失値\n",
    "plt.plot(sdnnr1.loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred1 = sdnnr1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035],\n",
       "       [10.93732035]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6553843111436706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MeanSquardError()\n",
    "m.mse(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 説明課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ドロップアウトとは何か\n",
    "ニューラルネットワークは過学習が発生しやすく、その解決方法として提案されたのがドロップアウトである。ドロップアウトは、学習中に中間層のユニットの値を一定の割合で0にし、結合を欠落させることで、ネットワークの締め付けを行って、過学習を抑制する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 近年ReLUが一般的に使われている理由\n",
    "ディープラーニングではモデルのパラメータを学習するために、誤差伝搬法という方法を繰り返し用いている。モデルから出力されたデータと解答を照らし合わせて,\n",
    "誤差を求めるが, その際、前回の誤差との差分に注目する。この時、誤差に微分値を使うが、ReLUを使うと、この微分は入力値が正の値であれば１を返し、誤差の微分を０にすることなく、次の学習へつなげることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重みの初期化方法について\n",
    "学習時、ネットワークのパラメータは乱数で初期化する。非常に深いネットワークの場合、初期化した乱数の値によってはバッチ正規化などのテクニックを利用しても収束しないことがある。学習をスムーズに進めるためには、単に乱数で初期化するのではなく、ある条件下において乱数で初期化することが大切である。   \n",
    "Xavierの初期化は、層のユニット数によって乱数の範囲を決める。Heの初期化は、活性化関数にReLUを用いることを想定し、各層におけるユニット数によって乱数の範囲を決める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代表的な最適化手法について\n",
    "・SGD（Stochastic Gradient Descent；確率的勾配降下法）：ミニバッチや逐次学習のように、学習サンプルの一部を用いて更新量を求める手法      \n",
    "・慣性項（モーメンタム）：前回の更新における勾配を反映させて勾配の振動を抑制し、収束性を改善する手法       \n",
    "・AdaGrad：学習におけるパラメータを更新するかを決めるハイパーパラメータである学習係数を自動で調整することができる手法    \n",
    "・Adam：過去の勾配の移動平均、過去の勾配の二乗の移動平均の両方を用いて、その偏りを補正した推定値を計算することによって、移動平均を用いたバイアスを打ち消すことができ、更新量の偏りを小さくする手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
