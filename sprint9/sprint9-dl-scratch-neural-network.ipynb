{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint9 NN(ニューラルネットワーク)の実装\n",
    "・スクラッチを通してニューラルネットワークの基礎を理解する     \n",
    "・基本的な深層学習のキーワードを学習する      \n",
    "・画像データの簡単な扱い方を知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# 保存先を指定\n",
    "mnist_dir = \"/Users/andoutakaaki/DIC_study/sprint9/mnist_data/\"\n",
    "\n",
    "# MNISTの読み込み\n",
    "mnist = fetch_mldata('MNIST original', data_home=mnist_dir)\n",
    "# trainとtestに分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.14285)\n",
    "# ラベルをint型にしておく\n",
    "y_train = y_train.astype(np.int)\n",
    "y_test = y_test.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "uint8\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0 123 186 239 183\n",
      " 226 244 158  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 203 254 254 254 254 254 254 226   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  54  95\n",
      "  95  95  95 196 254 228   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  61 252 254 123   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  48 232 254 204   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  28 231 254 234  29   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0 145 254 254 168  31   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  47 251 254 254 254\n",
      " 241 216  98   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  53 254 254 254 254 254 254 252 147   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  44 234 218\n",
      " 125 114  39 120 254 246  35   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  23  21   0   0   0  27 254 254  99   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  80 254 246  36   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14 224 254 227\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  16 159 254 252 107   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  17 187 254 254\n",
      " 189   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6\n",
      "   0   0   0   0   4  33 186 254 254 235  16   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0 133  51   0   3  31 158 254 254 254\n",
      " 243  81   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  70 236 182 198 255 254 254 254 239  82   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  22 211 254 254 255 254 254\n",
      " 194  41   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  21 131 216 254 163  68   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) # (60000, 784)\n",
    "print(X_test.shape) # (10000, 784)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape) \n",
    "print(y_test.shape) \n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練セットの最初の正解がでた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像データの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAD7JJREFUeJzt3X2sVHV+x/H3x6skjWLFKJTyoFuL\nqUUjq2AkmmqDa9U/qrRhI/0HQ/Sarg9rU02t/oFJu1bb3bWbNNVcAi5rfVisshprdRWND2kVLsQo\nLq6rBgW9vYiIgo21yLd/zKF7gTu/GWbOzBnu7/NKbu7MfOec83Xkc885cx5+igjMLD+HVd2AmVXD\n4TfLlMNvlimH3yxTDr9Zphx+s0w5/Ic4SZskXdDke0PS77a4nJantd7k8FvHSfoXSUOSPpf0tqQr\nq+7JHH7rjr8DToyIo4E/Bv5W0pkV95Q9h38MkXSWpP+UtKNY0/6TpHH7ve0SSe9J2ibpHyQdNmL6\nxZI2SvpU0tOSTiijr4h4MyL+Z+/T4uekMuZtrXP4x5avgb8AjgPmAvOA7+z3nvnAbOAM4FJgMYCk\ny4BbgD8BjgdeAh5sZqGSbpb0RIP3/LOk/wbeAoaAJ5v7T7JOkc/tP7RJ2gRcGRHPjlK7ATgvIuYX\nzwO4OCKeKp5/B/jTiJgn6d+Bf42IZUXtMGAXcEpEvF9MOyMi3mmj1z5qf5TOB+6MiP9tdV7WPq/5\nxxBJJ0t6QtJ/SfocuJ3aVsBIm0c8fh/47eLxCcCPil2GHcB2QMCUsvqLiK8j4mVgKvDnZc3XWuPw\njy13U9usnlF8uXYLtQCPNG3E4+nAR8XjzcDVEXHMiJ/fiIj/6ECfh+N9/so5/GPLeOBzYJek32P0\ntetNkiZImgZ8F/hp8fo9wF9Lmgkg6TclLWi3IUkTJV0u6ShJfZL+CFgIPNfuvK09Dv/YciPwZ8BO\nYCm/DvZIjwHrgNeAfwOWAUTEKuBO4KFil2EDcHEzC5V0S/GdwWiC2h+hLcCnwPeBGyLisSb/m6xD\n/IWfWaa85jfLlMNvlimH3yxTDr9Zpg7v5sKKs8TMrIMiYv9zO0bV1ppf0kWSfinpHUk3tzMvM+uu\nlg/1Fedpvw18i9ox3LXAwoj4RWIar/nNOqwba/6zgHci4r2I+Ap4iNpVYmZ2CGgn/FPY9yKRLYxy\nEYikfkmDkgbbWJaZlaydL/xG27Q4YLM+IgaAAfBmv1kvaWfNv4V9rxCbyq+vEDOzHtdO+NcCMyR9\no7hV1OXA4+W0ZWad1vJmf0TslnQt8DTQByyPiDdL68zMOqqrV/V5n9+s87pyko+ZHbocfrNMOfxm\nmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/\nWaYcfrNMOfxmmXL4zTLl8JtlqqtDdFtrbr311mT9vPPOq1ubM2dOctpXXnklWZ85c2ayPmHChGT9\n4Ycfrlu78cYbk9Nu3749Wbf2eM1vlimH3yxTDr9Zphx+s0w5/GaZcvjNMuXwm2XKo/QeAtauXZus\nn3nmmV3qpFxbtmxJ1qdPn96lTsaWZkfpbeskH0mbgJ3A18DuiJjdzvzMrHvKOMPvDyNiWwnzMbMu\n8j6/WabaDX8AP5e0TlL/aG+Q1C9pUNJgm8sysxK1u9l/TkR8JGki8IyktyLixZFviIgBYAD8hZ9Z\nL2lrzR8RHxW/twKrgLPKaMrMOq/l8Es6UtL4vY+BC4ENZTVmZp3Vzmb/JGCVpL3zeSAiniqlK9vH\nNddck6yfcsopXerkQFdeeWWyfs4559StFf92rCIthz8i3gNOL7EXM+siH+ozy5TDb5Yph98sUw6/\nWaYcfrNM+dbdh4A1a9a0VW/Hueeem6zPnTu35Xnfc889LU9r7fOa3yxTDr9Zphx+s0w5/GaZcvjN\nMuXwm2XK4TfLlG/dnblZs2Yl6089lb5Ke+LEicn6+vXr69bOPvvs5LS7d+9O1m10zd6622t+s0w5\n/GaZcvjNMuXwm2XK4TfLlMNvlimH3yxTPs4/xk2dOjVZbzT896RJk5L1bdvSY7Smhg/fvHlzclpr\njY/zm1mSw2+WKYffLFMOv1mmHH6zTDn8Zply+M0y5fv2j3E33XRTst7oOH4jzz33XLJ+2GFev/Sq\nhv9nJC2XtFXShhGvHSvpGUm/Kn5P6GybZla2Zv4s/xi4aL/XbgZWR8QMYHXx3MwOIQ3DHxEvAtv3\ne/lSYEXxeAVwWcl9mVmHtbrPPykihgAiYkhS3Ru5SeoH+ltcjpl1SMe/8IuIAWAAfGGPWS9p9avY\nYUmTAYrfW8trycy6odXwPw4sKh4vAh4rpx0z65aG1/NLehA4HzgOGAaWAD8DVgLTgQ+ABRGx/5eC\no83Lm/0dcPrpp9etDQ4OJqft6+sru5197Ny5s25taGgoOe2dd96ZrN97770t9TTWNXs9f8N9/ohY\nWKc076A6MrOe4tOvzDLl8JtlyuE3y5TDb5Yph98sU76kdww44ogj6tY6fSivkfHjx7dUA1i6dGmy\nfsYZZyTr1113XbKeO6/5zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMeYjuMeDUU0+tW2t0a+0d\nO3Yk6/fdd1+yvmvXrmT9ySefrFubP39+ctrbb789Wf/yyy+T9dNOO61u7d13301OeyjzEN1mluTw\nm2XK4TfLlMNvlimH3yxTDr9Zphx+s0z5OP8Y12gI7j179iTrH3/8cZnt7GPKlCnJ+ubNm9ua/1VX\nXVW3tmzZsrbm3ct8nN/Mkhx+s0w5/GaZcvjNMuXwm2XK4TfLlMNvlinft3+MGx4errqFuubMmdPW\n9I2u53/hhRfamv9Y13DNL2m5pK2SNox47TZJH0p6rfi5pLNtmlnZmtns/zFw0Siv3xURs4qf+rdr\nMbOe1DD8EfEisL0LvZhZF7Xzhd+1kl4vdgsm1HuTpH5Jg5IG21iWmZWs1fDfDZwEzAKGgB/Ue2NE\nDETE7IiY3eKyzKwDWgp/RAxHxNcRsQdYCpxVbltm1mkthV/S5BFP5wMb6r3XzHpTw+v5JT0InA8c\nBwwDS4rns4AANgFXR8RQw4X5ev7sHH300XVrr7/+enLa6dOnJ+sffvhhsj5t2rRkfaxq9nr+hif5\nRMTCUV4eu3dCMMuET+81y5TDb5Yph98sUw6/WaYcfrNM+ZJea8sxxxyTrN911111a40O5TW6rfgd\nd9yRrFua1/xmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaY8RLclTZhQ9w5tAKxcuTJZnzdvXsvL\nfv755zs277HMQ3SbWZLDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl4/wlGDduXLL+1VdfdamTAx1+\nePqWDVOnTk3WBwYGkvULLrjgoHva65NPPknWZ8yYkazv2LGj5WWPZT7Ob2ZJDr9Zphx+s0w5/GaZ\ncvjNMuXwm2XK4TfLVMP79kuaBvwE+C1gDzAQET+SdCzwU+BEasN0fzsiPu1cq71ryZIlyfqjjz6a\nrK9bty5Z7+vrS9ZTx+ob9XbFFVck6+364osv6tb6+/uT0/o4fmc1s+bfDfxlRJwCnA1cI+n3gZuB\n1RExA1hdPDezQ0TD8EfEUESsLx7vBDYCU4BLgRXF21YAl3WqSTMr30Ht80s6Efgm8CowKSKGoPYH\nAphYdnNm1jlNj9Un6SjgEeCGiPhcaur0YST1A+mdOzPruqbW/JKOoBb8+yNi77dXw5ImF/XJwNbR\npo2IgYiYHRGzy2jYzMrRMPyqreKXARsj4ocjSo8Di4rHi4DHym/PzDql4SW9ks4FXgLeoHaoD+AW\navv9K4HpwAfAgojY3mBeY/KS3gsvvDBZf+CBB5L1NWvWJOuNhsGeO3du3VqnL9n+7LPPkvXFixfX\nra1atarsdozmL+ltuM8fES8D9WbmG6ebHaJ8hp9Zphx+s0w5/GaZcvjNMuXwm2XK4TfLlG/dXYKJ\nE9OXNaxevTpZnzlzZlvLT51q3e7/35dffjlZb3RZ7ltvvdXW8u3g+dbdZpbk8JtlyuE3y5TDb5Yp\nh98sUw6/WaYcfrNM+Th/Fxx//PHJ+vXXX5+sL1iwIFk/+eST69buv//+5LQrVqxI1p999tlk3XqP\nj/ObWZLDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl4/xmY4yP85tZksNvlimH3yxTDr9Zphx+s0w5\n/GaZcvjNMtUw/JKmSXpe0kZJb0r6bvH6bZI+lPRa8XNJ59s1s7I0PMlH0mRgckSslzQeWAdcBnwb\n2BUR3296YT7Jx6zjmj3J5/AmZjQEDBWPd0raCExprz0zq9pB7fNLOhH4JvBq8dK1kl6XtFzShDrT\n9EsalDTYVqdmVqqmz+2XdBTwAvC9iHhU0iRgGxDA31DbNVjcYB7e7DfrsGY3+5sKv6QjgCeApyPi\nh6PUTwSeiIhTG8zH4TfrsNIu7FFtCNhlwMaRwS++CNxrPrDhYJs0s+o0823/ucBLwBvAnuLlW4CF\nwCxqm/2bgKuLLwdT8/Ka36zDSt3sL4vDb9Z5vp7fzJIcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yp\nh98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y1TDG3iWbBvw/ojnxxWv9aJe7a1X+wL31qoyezuh\n2Td29Xr+AxYuDUbE7MoaSOjV3nq1L3BvraqqN2/2m2XK4TfLVNXhH6h4+Sm92luv9gXurVWV9Fbp\nPr+ZVafqNb+ZVcThN8tUJeGXdJGkX0p6R9LNVfRQj6RNkt4ohh2vdHzBYgzErZI2jHjtWEnPSPpV\n8XvUMRIr6q0nhm1PDCtf6WfXa8Pdd32fX1If8DbwLWALsBZYGBG/6GojdUjaBMyOiMpPCJH0B8Au\n4Cd7h0KT9PfA9oi4o/jDOSEi/qpHeruNgxy2vUO91RtW/goq/OzKHO6+DFWs+c8C3omI9yLiK+Ah\n4NIK+uh5EfEisH2/ly8FVhSPV1D7x9N1dXrrCRExFBHri8c7gb3Dylf62SX6qkQV4Z8CbB7xfAsV\nfgCjCODnktZJ6q+6mVFM2jssWvF7YsX97K/hsO3dtN+w8j3z2bUy3H3Zqgj/aEMJ9dLxxnMi4gzg\nYuCaYvPWmnM3cBK1MRyHgB9U2UwxrPwjwA0R8XmVvYw0Sl+VfG5VhH8LMG3E86nARxX0MaqI+Kj4\nvRVYRW03pZcM7x0hufi9teJ+/l9EDEfE1xGxB1hKhZ9dMaz8I8D9EfFo8XLln91ofVX1uVUR/rXA\nDEnfkDQOuBx4vII+DiDpyOKLGCQdCVxI7w09/jiwqHi8CHiswl720SvDttcbVp6KP7teG+6+kjP8\nikMZ/wj0Acsj4ntdb2IUkn6H2toeapc7P1Blb5IeBM6ndsnnMLAE+BmwEpgOfAAsiIiuf/FWp7fz\nOchh2zvUW71h5V+lws+uzOHuS+nHp/ea5cln+JllyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmfo/\ngsfAmzqY+8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngsfAmzqY+8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35   17.65   80.65  133.65   77.65  120.65  138.65   52.65  -87.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35   97.65  148.65  148.65  148.65  148.65  148.65  148.65  120.65\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35  -51.35  -10.35  -10.35  -10.35  -10.35   90.65  148.65  122.65\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35  -44.35  146.65  148.65   17.65\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35  -57.35  126.65  148.65   98.65 -101.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35  -77.35  125.65  148.65  128.65  -76.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35   39.65  148.65  148.65   62.65  -74.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35  -58.35  145.65  148.65  148.65  148.65  135.65  110.65\n",
      "    -7.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35  -52.35  148.65  148.65  148.65  148.65  148.65  148.65\n",
      "   146.65   41.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35  -61.35  128.65  112.65   19.65    8.65  -66.35   14.65\n",
      "   148.65  140.65  -70.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35  -82.35  -84.35 -105.35 -105.35 -105.35  -78.35\n",
      "   148.65  148.65   -6.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
      "   148.65  140.65  -69.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -91.35  118.65\n",
      "   148.65  121.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35   53.65  148.65\n",
      "   146.65    1.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35  -88.35   81.65  148.65  148.65\n",
      "    83.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -99.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -101.35  -72.35   80.65  148.65  148.65  129.65\n",
      "   -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35   27.65  -54.35\n",
      "  -105.35 -102.35  -74.35   52.65  148.65  148.65  148.65  137.65  -24.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -35.35  130.65\n",
      "    76.65   92.65  149.65  148.65  148.65  148.65  133.65  -23.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -83.35  105.65\n",
      "   148.65  148.65  149.65  148.65  148.65   88.65  -64.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -84.35\n",
      "    25.65  110.65  148.65   57.65  -37.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "image = image.astype(np.float) # float型に変換\n",
    "image -= 105.35 # 意図的に負の小数値を作り出してみる\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()\n",
    "print(image) # 値を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "画像は0から255のuint8型で表されたが、機械学習をする上では0から1のfloat型に変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解ラベルは0から9の整数であるが、ニューラルネットワークで多クラス分類を行う際にはone-hot表現に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(48000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重みの初期値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_features = 784\n",
    "# n_node1 = 400\n",
    "# sigma = 0.01  # ガウス分布の標準偏差\n",
    "# W1 = sigma * np.random.randn(n_features, n_node1)\n",
    "# # W1: (784, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# １層目の重み（行７８４、列４００）\n",
    "n_features1 = 784\n",
    "n_node1 = 400\n",
    "sigma = 0.01\n",
    "\n",
    "w1 = sigma*np.random.randn(n_features1, n_node1)\n",
    "b1 = sigma*np.random.randn(n_node1)\n",
    "\n",
    "# ２層目の重み（行４００、列２００）\n",
    "n_features2 = 400\n",
    "n_node2 = 200\n",
    "\n",
    "w2 = sigma*np.random.randn(n_features2, n_node2)\n",
    "b2 = sigma*np.random.randn(n_node2)\n",
    "\n",
    "# 3層目の重み（行２００、列１０）\n",
    "n_features3 = 200\n",
    "n_node3 = 10\n",
    "\n",
    "w3 = sigma*np.random.randn(n_features3, n_node3)\n",
    "b3 = sigma*np.random.randn(n_node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)または(n_samples,)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "\n",
    "    Retruns\n",
    "    ----------\n",
    "    for文で呼び出すと以下の2つを返す。最後のイテレーションでは、バッチサイズより小さいこともある。\n",
    "    mini_X : 次の形のndarray, shape (batch_size, n_features)\n",
    "      学習データのミニバッチ\n",
    "    mini_y : 次の形のndarray, shape (batch_size, 1)または(batch_size,)\n",
    "      正解値のミニバッチ\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        # ランダムに並べ換える\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._counter = 0\n",
    "        # イテレーション数を計算する\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        # len()が使われたときの処理\n",
    "        return self._stop\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # for文で呼ばれた際のループごとの処理\n",
    "        if self._counter >= self._stop:\n",
    "            # 最後まで進んだら終了\n",
    "            self._counter = 0 # カウンターをリセット\n",
    "            raise StopIteration()\n",
    "\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクラスをニューラルネットワークのクラス内でインスタンス化し、for文を使うことでミニバッチが取り出せる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n"
     ]
    }
   ],
   "source": [
    "# 以下をニューラルネットワークのクラス内で呼び出す\n",
    "\n",
    "get_mini_batch = GetMiniBatch(X_train, y_train_one_hot, batch_size=10)\n",
    "\n",
    "print(len(get_mini_batch)) # 4800\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    # このfor文内でミニバッチが使える\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y_pred, y_train):\n",
    "    if y_pred.ndim == 1:\n",
    "        y_train = y_train.reshape(1, y_train.size)\n",
    "        y_pred = y_pred.reshape(1, y_pred.size)\n",
    "    \n",
    "    batch_size = y_pred.shape[0]\n",
    "    return -np.sum(y_train * np.log(y_pred)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習（フォワード、バックプロパゲーション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    get_mini_batch = GetMiniBatch(X_train, y_train_one_hot, batch_size=10, seed=0)\n",
    "\n",
    "    loss_list = []\n",
    "    c = 0\n",
    "    for mini_X_train, mini_y_train in get_mini_batch:\n",
    "        # 1層目\n",
    "        rayer1_pre = np.dot(mini_X_train, w1) + b1\n",
    "        rayer1_out = sigmoid(rayer1_pre)\n",
    "\n",
    "        # 2層目\n",
    "        rayer2_pre = np.dot(rayer1_out, w2) + b2\n",
    "        rayer2_out = sigmoid(rayer2_pre)\n",
    "\n",
    "        # 3層目\n",
    "        rayer3_pre = np.dot(rayer2_out, w3) + b3\n",
    "        rayer3_out = softmax(rayer3_pre)\n",
    "        # print(rayer3_out.shape)\n",
    "\n",
    "        # 損失関数で誤差を算出する\n",
    "        loss = cross_entropy_error(rayer3_out, mini_y_train)\n",
    "        # print(loss)\n",
    "        c = c+1\n",
    "\n",
    "        # def backprob(mini_X_train, mini_y_train, rayer3_out , w1, w2,w3,b1, b2, b3):\n",
    "        delta1 = rayer3_out - mini_y_train\n",
    "        delta2 = (1 - np.tanh(rayer2_pre)**2)*(delta1.dot(w3.T))\n",
    "        delta3 = (1 - np.tanh(rayer1_pre)**2)*(delta2.dot(w2.T))\n",
    "\n",
    "        b3_grad = delta1\n",
    "        w3_grad = rayer2_out.T.dot(delta1) \n",
    "\n",
    "        b2_grad = delta2\n",
    "        w2_grad = rayer1_out.T.dot(delta2) \n",
    "\n",
    "        b1_grad = delta3\n",
    "        w1_grad = mini_X_train.T.dot(delta3) \n",
    "            # return delta2, delta3   \n",
    "\n",
    "        alpha = 0.001\n",
    "        w1= w1 - alpha*w1_grad\n",
    "        w2= w2 - alpha*w2_grad\n",
    "        w3= w3 - alpha*w3_grad\n",
    "        b1 = b1 - alpha*b1_grad.sum(axis=0)\n",
    "        b2 = b2 - alpha*b2_grad.sum(axis=0)\n",
    "        b3 = b3 - alpha*b3_grad.sum(axis=0)\n",
    "\n",
    "\n",
    "        loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.4953192879196635e-05,\n",
       " 0.00010182086150800123,\n",
       " 7.8648505077271656e-05,\n",
       " 0.0051962007700655979,\n",
       " 0.0057896719447239394,\n",
       " 0.00017203737551868062,\n",
       " 0.0023913061605395449,\n",
       " 0.00033074560129671052,\n",
       " 0.002817064104387442,\n",
       " 0.00016012293572331091,\n",
       " 0.00080289337882804689,\n",
       " 0.0080668661472311533,\n",
       " 0.00025314659274383702,\n",
       " 0.00069322867140869573,\n",
       " 0.0022481843751655283,\n",
       " 0.001952926897593827,\n",
       " 0.00075895739097846693,\n",
       " 0.0016691594298295352,\n",
       " 0.0002262874874310795,\n",
       " 0.00064642454890624342,\n",
       " 0.00011147747723192062,\n",
       " 0.00053563554929467096,\n",
       " 0.00021144142278549253,\n",
       " 0.0028095939370925268,\n",
       " 0.0019814017225124833,\n",
       " 0.0019495079115355053,\n",
       " 5.4776981039312956e-05,\n",
       " 0.00086858763200408739,\n",
       " 0.0001437899916969433,\n",
       " 0.0022344384600214372,\n",
       " 0.00045041917829992455,\n",
       " 0.00036317924500569107,\n",
       " 0.0074528052974797932,\n",
       " 0.0010898725690933887,\n",
       " 0.00077775703463999748,\n",
       " 0.00089145126327848845,\n",
       " 0.00011555420777431391,\n",
       " 0.00036025305411980623,\n",
       " 1.9547099870388012e-05,\n",
       " 0.0039851135876797592,\n",
       " 0.00011110843687340203,\n",
       " 0.0047052236571411875,\n",
       " 0.0066209012033501408,\n",
       " 6.3002582242950662e-05,\n",
       " 4.3296370531608854e-05,\n",
       " 0.0012535475555041325,\n",
       " 0.0028809533906218499,\n",
       " 3.4725888244494318e-05,\n",
       " 0.00037556417364843471,\n",
       " 0.00010683570628057323,\n",
       " 0.0002688965452342369,\n",
       " 7.1101366972594423e-06,\n",
       " 0.0027917968789141377,\n",
       " 0.0016150970306380217,\n",
       " 0.0072190420686842824,\n",
       " 0.0013158681432595578,\n",
       " 0.0012633127160680841,\n",
       " 0.002814641464802382,\n",
       " 0.00135712335921489,\n",
       " 0.0055371068283580826,\n",
       " 0.0017228740746237671,\n",
       " 0.00065176598316294538,\n",
       " 0.0085670835716481729,\n",
       " 0.00090672646194975455,\n",
       " 0.0001114682504793127,\n",
       " 0.00049382140294121325,\n",
       " 9.9902594483034489e-06,\n",
       " 0.0022071624580781266,\n",
       " 0.0005033543083178847,\n",
       " 0.00059653044181777113,\n",
       " 0.00014035959871399895,\n",
       " 0.0010371224580938848,\n",
       " 0.0019994876960386342,\n",
       " 0.00019336462005147662,\n",
       " 0.0063863940532434566,\n",
       " 0.0015549105079929724,\n",
       " 0.00085152174951068588,\n",
       " 9.6848011878216149e-05,\n",
       " 0.00070038851322074133,\n",
       " 0.0003507841727944795,\n",
       " 0.0001633638208633905,\n",
       " 0.0008387290567114543,\n",
       " 0.00068451568100893903,\n",
       " 0.00014322409273005777,\n",
       " 0.0052898935327995531,\n",
       " 0.0028211311396794643,\n",
       " 0.0013204196495304565,\n",
       " 0.00025393368832455233,\n",
       " 0.00044294948790959217,\n",
       " 0.00061400321078012806,\n",
       " 0.00017545139813431157,\n",
       " 0.0003873103042462987,\n",
       " 0.0045872176745159676,\n",
       " 0.0046432734890630139,\n",
       " 0.00033422938694615346,\n",
       " 8.0837066810910874e-05,\n",
       " 0.0053379072263502001,\n",
       " 1.563147154943393e-05,\n",
       " 0.0011875504201783254,\n",
       " 0.0010335983909375427,\n",
       " 0.00086478253505009302,\n",
       " 0.0027966687223965566,\n",
       " 0.0030623435391168349,\n",
       " 0.010306317620561523,\n",
       " 0.0007428384556429046,\n",
       " 0.00026673429970320146,\n",
       " 0.0015632989121401649,\n",
       " 0.00075348413968147137,\n",
       " 0.00091153638108393688,\n",
       " 0.00020825529125427801,\n",
       " 1.2556701745151581e-05,\n",
       " 0.0037720940293614405,\n",
       " 3.3444537450944481e-05,\n",
       " 0.00013092257490604874,\n",
       " 6.400001061772096e-05,\n",
       " 0.00091171724036134426,\n",
       " 0.0005108321236281519,\n",
       " 0.0019145813086238392,\n",
       " 4.0363140915651089e-05,\n",
       " 0.00096898926022156062,\n",
       " 0.0016806045364750517,\n",
       " 0.00064171269791184953,\n",
       " 5.5980020229569894e-05,\n",
       " 0.00043978865171572662,\n",
       " 0.0013665411519730676,\n",
       " 0.0066216056619040013,\n",
       " 0.00018536455280695387,\n",
       " 0.00016563607819850375,\n",
       " 0.00034873237197184095,\n",
       " 6.0459970943187367e-05,\n",
       " 0.00049726606480122158,\n",
       " 6.7939725228549406e-06,\n",
       " 0.0003655389518131295,\n",
       " 3.9496498740825117e-05,\n",
       " 0.0050626096632871934,\n",
       " 0.0042592169573244174,\n",
       " 0.000602948898951527,\n",
       " 0.0017887091762078093,\n",
       " 0.00011613724553657519,\n",
       " 0.00065457578053012308,\n",
       " 0.0002247012646258006,\n",
       " 4.9463329458508838e-05,\n",
       " 0.0010355100876684492,\n",
       " 0.0013153928472328274,\n",
       " 0.0021675222639039528,\n",
       " 0.00024944112689860999,\n",
       " 0.00074066577466190553,\n",
       " 2.4853901318027636e-05,\n",
       " 0.00071473776394785085,\n",
       " 0.00013689690993650219,\n",
       " 0.0012091000594584493,\n",
       " 0.00013022270482763448,\n",
       " 0.0001185512971754794,\n",
       " 0.0001586980657575263,\n",
       " 0.0028739533650932541,\n",
       " 0.00017448862259082556,\n",
       " 0.0017794028210652677,\n",
       " 0.00052097493279857916,\n",
       " 0.00037792244481340046,\n",
       " 0.00082586911625870711,\n",
       " 1.1015471217293145e-05,\n",
       " 0.00075321312959484416,\n",
       " 0.00056120078814642874,\n",
       " 0.0019121450903212506,\n",
       " 0.0013045173051504963,\n",
       " 0.0018242270116421099,\n",
       " 0.0023582520431349764,\n",
       " 0.0015925609548012489,\n",
       " 0.0012280610695355027,\n",
       " 0.0011962316072923311,\n",
       " 0.0060240456747125823,\n",
       " 0.0024097208455658653,\n",
       " 0.0029471757122100893,\n",
       " 0.00023906271877754509,\n",
       " 0.00012621561642432141,\n",
       " 0.00077866427832704358,\n",
       " 0.00048805077408911027,\n",
       " 2.5369228568810265e-05,\n",
       " 0.0050626931842455564,\n",
       " 0.00052568112444994369,\n",
       " 0.004065706764748855,\n",
       " 0.01260442720084518,\n",
       " 0.0040800739258607371,\n",
       " 0.0001762137150435549,\n",
       " 0.0017107455339955477,\n",
       " 0.0037578106963676201,\n",
       " 0.00033437642102952906,\n",
       " 0.00028064251335185397,\n",
       " 8.8885995329550754e-05,\n",
       " 0.00010722169124067159,\n",
       " 0.0039528267304766085,\n",
       " 0.00025509951830454149,\n",
       " 0.00036794958899311563,\n",
       " 0.00030659838683763115,\n",
       " 0.0027835476348471613,\n",
       " 0.0023542619381837398,\n",
       " 1.9015363780548565e-05,\n",
       " 0.0001886539071781444,\n",
       " 0.00053567385873481727,\n",
       " 0.0036000345588171993,\n",
       " 0.0028222437028650937,\n",
       " 4.828275627551519e-06,\n",
       " 0.0024251878029805129,\n",
       " 0.00094965901888950947,\n",
       " 0.00090100306909162917,\n",
       " 0.0036107696155053227,\n",
       " 0.00025565218206736741,\n",
       " 0.0021163919751469229,\n",
       " 0.0017733630654969683,\n",
       " 0.0022854220513686763,\n",
       " 0.0027800436897771137,\n",
       " 0.00083942457814599936,\n",
       " 0.00044997248723601817,\n",
       " 7.074706531475559e-05,\n",
       " 0.00053081985449753856,\n",
       " 0.0030149295928653596,\n",
       " 1.0403053065954199e-05,\n",
       " 0.00099874541622218269,\n",
       " 0.0012279978935306672,\n",
       " 8.3539851033202718e-05,\n",
       " 6.0693102272393723e-05,\n",
       " 0.0012921776704896506,\n",
       " 0.0018194266362044874,\n",
       " 0.008040773166927296,\n",
       " 0.006192785529086052,\n",
       " 0.0041783621273110572,\n",
       " 0.00067693510258198602,\n",
       " 9.163650929293221e-05,\n",
       " 0.00021927464687331388,\n",
       " 0.0004624510672584737,\n",
       " 0.0011496468030906099,\n",
       " 4.7976739970877178e-05,\n",
       " 7.652902088058315e-05,\n",
       " 0.0047771559621076452,\n",
       " 0.00016383583761390465,\n",
       " 0.0017456636505365935,\n",
       " 0.0012090628342714181,\n",
       " 0.00018317358017177238,\n",
       " 0.0047945709477867967,\n",
       " 0.00029797080248342991,\n",
       " 1.2518358328809315e-05,\n",
       " 0.00053750862954467823,\n",
       " 0.004915046888261566,\n",
       " 0.00094131721059025411,\n",
       " 0.0024633790357715358,\n",
       " 0.0012450587938507437,\n",
       " 0.0042150987095577221,\n",
       " 0.0013216104054213345,\n",
       " 0.0015331477510672229,\n",
       " 0.00516713833971398,\n",
       " 0.0029953201566910582,\n",
       " 0.000615490631188456,\n",
       " 0.0042354003475227305,\n",
       " 0.00032174563199444567,\n",
       " 0.0045744021408410463,\n",
       " 2.2563417884510209e-05,\n",
       " 0.001258483425651304,\n",
       " 0.0010933643227880022,\n",
       " 0.00020778479771022042,\n",
       " 4.4479525130257758e-05,\n",
       " 0.00024200352416104673,\n",
       " 0.0037527021917866041,\n",
       " 0.010873977935910245,\n",
       " 0.00011376523402109283,\n",
       " 0.00091203114514759457,\n",
       " 0.00047612190885872551,\n",
       " 0.0021054502040795081,\n",
       " 4.16739035155141e-05,\n",
       " 0.00091631971727113015,\n",
       " 2.4560774388044075e-05,\n",
       " 0.0037231901597583163,\n",
       " 0.0013423021950552869,\n",
       " 0.0031782199638947699,\n",
       " 5.5348286294026517e-05,\n",
       " 8.5067832173867336e-05,\n",
       " 0.0021039087780080046,\n",
       " 0.0020807191866408425,\n",
       " 0.00017308644266709566,\n",
       " 0.0089265694467246352,\n",
       " 0.0022507786329503628,\n",
       " 0.0015132413682148143,\n",
       " 6.0586202759751852e-05,\n",
       " 0.002050226303627297,\n",
       " 2.1155132800428721e-05,\n",
       " 0.0029289315186627426,\n",
       " 0.00010207597175979561,\n",
       " 0.0027402638614926426,\n",
       " 0.0007164333645037637,\n",
       " 0.00017581879398266331,\n",
       " 0.00037578366992410548,\n",
       " 7.7627153697502758e-05,\n",
       " 0.00016895589807295323,\n",
       " 4.4320347429219497e-05,\n",
       " 0.0099821139255719202,\n",
       " 2.4474119632575879e-05,\n",
       " 0.00012260722133783817,\n",
       " 0.0011463859438060775,\n",
       " 0.0013529391215180563,\n",
       " 0.00068713019508210844,\n",
       " 0.0020365499117622589,\n",
       " 0.00048021327889493997,\n",
       " 0.0010913068922311395,\n",
       " 0.0017481125057148758,\n",
       " 1.9132023603080908e-05,\n",
       " 0.001998700029192905,\n",
       " 0.00039432971491146825,\n",
       " 0.0019298622528610698,\n",
       " 0.0029126095101089161,\n",
       " 0.00012227320741643665,\n",
       " 0.00096089681947864709,\n",
       " 0.00019406218780204136,\n",
       " 0.0057923570769618411,\n",
       " 0.0083003536724823683,\n",
       " 0.0031949979625213328,\n",
       " 0.0021000733066894191,\n",
       " 0.00044638372669751614,\n",
       " 0.00018753764161892682,\n",
       " 0.0083157617856459247,\n",
       " 0.00030844396719624599,\n",
       " 0.0011819204655405511,\n",
       " 0.00074233606916217709,\n",
       " 0.0036015527179418727,\n",
       " 0.00048595420250103485,\n",
       " 3.5182489908090484e-05,\n",
       " 0.0025495961651476008,\n",
       " 5.8076304292560859e-05,\n",
       " 0.0021413265331937832,\n",
       " 0.0002450565645720573,\n",
       " 0.00058432219420233582,\n",
       " 0.0023866578595262032,\n",
       " 6.4574521885811919e-05,\n",
       " 2.6013497293751269e-05,\n",
       " 0.00019958495139674927,\n",
       " 0.0026388752822868308,\n",
       " 0.00075903482811951622,\n",
       " 0.0003757606512219725,\n",
       " 3.1001283554559549e-05,\n",
       " 0.00015045674959447537,\n",
       " 0.0050245110529592522,\n",
       " 0.0045875833272484238,\n",
       " 0.00074954602838539388,\n",
       " 0.00024334754583553989,\n",
       " 0.0013293221188288647,\n",
       " 0.00073848773978682448,\n",
       " 0.0003197072209431696,\n",
       " 0.0049300109619725774,\n",
       " 0.00015239699766120336,\n",
       " 0.012472801880504931,\n",
       " 0.0012820723700809346,\n",
       " 0.00027724365357680519,\n",
       " 7.5962784514790982e-05,\n",
       " 6.5787503003205208e-05,\n",
       " 0.0014528519491846974,\n",
       " 4.4934307438286187e-05,\n",
       " 0.00016393833231712537,\n",
       " 0.00037678574708869021,\n",
       " 0.0019624825692840949,\n",
       " 0.00052670556490635816,\n",
       " 0.00022620903095514137,\n",
       " 0.00022476196735808163,\n",
       " 0.001212821017049309,\n",
       " 0.0097384664479475917,\n",
       " 0.0011481003376366745,\n",
       " 5.871263941891877e-05,\n",
       " 0.0028544580397240381,\n",
       " 4.82102400666493e-05,\n",
       " 0.0028447559007144891,\n",
       " 0.00022337200439569155,\n",
       " 0.00019405775295324343,\n",
       " 0.0029188788265924913,\n",
       " 0.0008120627046494369,\n",
       " 0.0026785452486302679,\n",
       " 0.006757946475316255,\n",
       " 0.014091816835698814,\n",
       " 0.0037336936456631295,\n",
       " 0.003910290656546485,\n",
       " 0.0029689744896223731,\n",
       " 0.002193966767826943,\n",
       " 0.0025463914855418249,\n",
       " 0.00070861782021691439,\n",
       " 0.00040914162762091132,\n",
       " 0.00035320328056842945,\n",
       " 0.00023469968546385497,\n",
       " 0.00010893645017012706,\n",
       " 0.0052770389739450656,\n",
       " 0.00010350984066772998,\n",
       " 0.00025037260473260834,\n",
       " 0.0045201060776807441,\n",
       " 0.0043048171974676601,\n",
       " 0.00030374299489640562,\n",
       " 0.00022071747428921594,\n",
       " 0.00033377003237388992,\n",
       " 0.0018833226482198463,\n",
       " 0.00067868433598079433,\n",
       " 6.6075200472823367e-05,\n",
       " 0.00010320341728596022,\n",
       " 0.0010007083742749495,\n",
       " 0.0022548161919926928,\n",
       " 0.0006198541394919618,\n",
       " 0.0014982323838911161,\n",
       " 0.00044396830037635751,\n",
       " 0.00011498181882199675,\n",
       " 0.0017818466953303292,\n",
       " 0.00029728185669808383,\n",
       " 0.0057149543257179834,\n",
       " 0.00035667946623669014,\n",
       " 5.1035316951020193e-05,\n",
       " 1.0014074898098763e-05,\n",
       " 0.00070664024023452663,\n",
       " 0.00011229736665839386,\n",
       " 0.00079827038729057657,\n",
       " 0.0010029764869143947,\n",
       " 0.00049245775170544319,\n",
       " 0.0038657231639338554,\n",
       " 0.00031199855223470552,\n",
       " 0.0024793590775949023,\n",
       " 0.00015765249042399792,\n",
       " 0.0014792476881114926,\n",
       " 0.00030437369252282162,\n",
       " 0.0038761344486490392,\n",
       " 0.0011222876010357566,\n",
       " 0.00095155609256178755,\n",
       " 0.0009399833567432566,\n",
       " 0.00033585065533299805,\n",
       " 0.00054967182724550783,\n",
       " 0.00038442980867608865,\n",
       " 0.00085437259562969308,\n",
       " 0.0015150958529173996,\n",
       " 2.5601680463205533e-06,\n",
       " 0.00029392000074006332,\n",
       " 0.00093516679355589078,\n",
       " 0.0001513535901912149,\n",
       " 0.00013371324857756577,\n",
       " 0.00037738884195344998,\n",
       " 0.0014675575039035502,\n",
       " 0.00011806806464691485,\n",
       " 0.014349124349105494,\n",
       " 0.0016385237015095161,\n",
       " 0.0025374102007618823,\n",
       " 0.0010697268826771488,\n",
       " 0.0019301685744027633,\n",
       " 0.0013885873953786211,\n",
       " 0.000741997319246719,\n",
       " 0.0003977885736484542,\n",
       " 0.0010105769307203433,\n",
       " 2.5360209490496776e-05,\n",
       " 0.0012992379942605673,\n",
       " 0.0013434136008676379,\n",
       " 6.2674738574054566e-05,\n",
       " 0.00071853063897159915,\n",
       " 0.0045549989219920272,\n",
       " 0.00013622211802945299,\n",
       " 0.00028409778189404281,\n",
       " 0.0016092097031545362,\n",
       " 0.00012391757711163347,\n",
       " 2.8009198353313947e-05,\n",
       " 0.00020390690218733586,\n",
       " 0.0007257207360933214,\n",
       " 0.0022919023618958555,\n",
       " 0.00015202791559939963,\n",
       " 3.7702286101548186e-05,\n",
       " 0.0049493210341033395,\n",
       " 0.00056056647836052662,\n",
       " 1.0295186042353746e-05,\n",
       " 0.00095450274845873648,\n",
       " 0.00022345810874108466,\n",
       " 9.3842447366312601e-05,\n",
       " 0.0017937522875376348,\n",
       " 0.00123985542766227,\n",
       " 2.6760382370376069e-05,\n",
       " 9.1507929502985407e-05,\n",
       " 0.00024449684158741128,\n",
       " 3.1524271566177862e-05,\n",
       " 0.00086883057290487677,\n",
       " 0.0038868776830071538,\n",
       " 0.00050477420818335797,\n",
       " 0.00060264841990510429,\n",
       " 0.0057143563086950932,\n",
       " 0.00071422329183470055,\n",
       " 0.0021100225406301739,\n",
       " 0.0069762145731560032,\n",
       " 0.0027690081688332901,\n",
       " 0.00017678554980125251,\n",
       " 0.0012187175514091644,\n",
       " 0.00080841212460185054,\n",
       " 0.0015921942000131604,\n",
       " 0.0058973070321639235,\n",
       " 0.00097843505007501027,\n",
       " 0.00059619341574936242,\n",
       " 0.0015386843133841815,\n",
       " 0.00552449404275025,\n",
       " 0.0011904957985843085,\n",
       " 0.0033465923974341956,\n",
       " 0.0078203480175495152,\n",
       " 0.0024325966979677426,\n",
       " 0.0018925202754325268,\n",
       " 2.5728282471346649e-05,\n",
       " 0.00054660167393427966,\n",
       " 0.00073606164344695402,\n",
       " 0.0051641912820702077,\n",
       " 0.0026355035573861732,\n",
       " 0.0067663897245889906,\n",
       " 0.0041977077286362201,\n",
       " 8.458123553064514e-05,\n",
       " 0.00078952231478207602,\n",
       " 0.0010781287895955602,\n",
       " 0.0046782500362138671,\n",
       " 0.00050046021614052669,\n",
       " 0.0035290652276201775,\n",
       " 0.0013233904732648518,\n",
       " 0.00042569770461102117,\n",
       " 0.010847519566039189,\n",
       " 0.00035596964038693622,\n",
       " 0.0048873342821067157,\n",
       " 0.0016346263622024088,\n",
       " 0.0014244893076886885,\n",
       " 0.0043431993439642328,\n",
       " 0.00028100879640063296,\n",
       " 0.00071179469857227248,\n",
       " 0.0020643576802141287,\n",
       " 0.0015179818401526467,\n",
       " 0.00053696718261346326,\n",
       " 4.4832672344183543e-05,\n",
       " 0.0042505747685645983,\n",
       " 0.00062006041755088869,\n",
       " 0.00018193669617204682,\n",
       " 5.2569699630556842e-05,\n",
       " 4.8221242431979864e-05,\n",
       " 0.002994666515517838,\n",
       " 0.00010986641295129987,\n",
       " 0.0011765212952350587,\n",
       " 0.0019607193901419969,\n",
       " 0.00014547876735011267,\n",
       " 0.0066245217988740029,\n",
       " 0.0003313776576188983,\n",
       " 0.0027339633739216136,\n",
       " 0.00059409979987970073,\n",
       " 0.005513093846199124,\n",
       " 0.0025031772414282711,\n",
       " 9.7893773385867872e-05,\n",
       " 0.0012779452578242615,\n",
       " 5.45595373077574e-05,\n",
       " 0.00080422785335507987,\n",
       " 0.00018798677928849646,\n",
       " 9.6759150966953063e-05,\n",
       " 2.3034457899745129e-05,\n",
       " 0.0016039648125451336,\n",
       " 2.3274379559650543e-05,\n",
       " 0.000431401150888801,\n",
       " 0.011413537762586513,\n",
       " 0.0059991978948894676,\n",
       " 0.0042611807885770143,\n",
       " 0.0035020021363029916,\n",
       " 0.00086879914087129668,\n",
       " 0.0049343256730357901,\n",
       " 0.0011507971539002587,\n",
       " 2.0455520670853817e-05,\n",
       " 0.00013694582883116247,\n",
       " 0.00047615046917468668,\n",
       " 0.00015648450790773421,\n",
       " 0.00048731318312000675,\n",
       " 5.0069344315343277e-05,\n",
       " 0.0085888064230628305,\n",
       " 0.0028322806554280247,\n",
       " 0.0081204103847510745,\n",
       " 0.00026358245989135763,\n",
       " 9.5262966660442132e-05,\n",
       " 0.003893276231702355,\n",
       " 0.00043149890106208296,\n",
       " 0.0023721317843226246,\n",
       " 0.0025661859774664059,\n",
       " 0.00058819063982300766,\n",
       " 0.00036238117090376775,\n",
       " 0.00043284752889565078,\n",
       " 0.00052282712733331773,\n",
       " 0.00024715249148293762,\n",
       " 0.0024004558311759789,\n",
       " 0.0032197164511895181,\n",
       " 0.00080336314617343845,\n",
       " 0.0014195357351638742,\n",
       " 0.0010484158345769495,\n",
       " 0.00038600120109657078,\n",
       " 0.0021634969027301055,\n",
       " 0.00063329231727025589,\n",
       " 0.0019593472193398056,\n",
       " 0.00066010489276997113,\n",
       " 0.0026185711973287437,\n",
       " 0.0022363732365640689,\n",
       " 0.0023613962324880678,\n",
       " 0.0047706438787358581,\n",
       " 0.0021892662311150711,\n",
       " 0.0034014958707228307,\n",
       " 0.00036383554265526579,\n",
       " 0.0030131767739746008,\n",
       " 0.0028691769023092553,\n",
       " 0.011543869580249191,\n",
       " 0.00021897667478011754,\n",
       " 9.8452645984326102e-05,\n",
       " 0.00059540681779797575,\n",
       " 7.4529025057360488e-05,\n",
       " 0.0058961565819097613,\n",
       " 0.0055574889154970638,\n",
       " 0.00045252063747285357,\n",
       " 7.8003492965216708e-05,\n",
       " 0.0026943660305477858,\n",
       " 0.0071215232192300831,\n",
       " 0.00031243999134599777,\n",
       " 0.0026156041312843993,\n",
       " 0.00018794884350312608,\n",
       " 0.00012233997487362698,\n",
       " 5.194134185517711e-05,\n",
       " 0.0042909108149333137,\n",
       " 3.8000663836320626e-05,\n",
       " 0.0010244187946391024,\n",
       " 0.00090085967799294721,\n",
       " 0.0040047236858305451,\n",
       " 9.8863815028875937e-05,\n",
       " 3.1520745106613734e-05,\n",
       " 0.0022782451452261938,\n",
       " 0.0038877299699004225,\n",
       " 0.0025096368678115934,\n",
       " 0.0018643149560893032,\n",
       " 0.00054711943068252837,\n",
       " 0.0055304552444839471,\n",
       " 0.0052477927562306738,\n",
       " 5.5433056633286157e-05,\n",
       " 0.0048971244138635382,\n",
       " 0.0083941217513949454,\n",
       " 0.0033844503331591932,\n",
       " 0.00051958360109959068,\n",
       " 0.00060058998988530004,\n",
       " 7.8836237514455087e-05,\n",
       " 0.0011951752511179935,\n",
       " 0.00084112766287996099,\n",
       " 0.0021658192543397487,\n",
       " 0.00036376521119790102,\n",
       " 0.00064051651436344218,\n",
       " 0.0023731985122896143,\n",
       " 0.0021104702900297074,\n",
       " 0.0027046184459285859,\n",
       " 0.0010679442538912378,\n",
       " 0.00072544464287716817,\n",
       " 0.0012125952009251133,\n",
       " 0.00017661525089462731,\n",
       " 0.0015642002024842112,\n",
       " 0.00096059621901168073,\n",
       " 0.0014153348733977598,\n",
       " 0.0041858654423896201,\n",
       " 9.5020396948743862e-05,\n",
       " 0.0021401416440564457,\n",
       " 0.0042268833080136795,\n",
       " 0.0037825174376255612,\n",
       " 0.00060710454185826379,\n",
       " 0.00061624935611860935,\n",
       " 0.00070508806593069014,\n",
       " 0.0005915051714709739,\n",
       " 0.0064786813617882869,\n",
       " 0.0013634243624205127,\n",
       " 0.001276814225431473,\n",
       " 0.00099616009853747208,\n",
       " 0.00038049938394179374,\n",
       " 0.00097267461910606012,\n",
       " 0.002491480335621086,\n",
       " 6.6586280063421526e-05,\n",
       " 6.4642764873049698e-05,\n",
       " 0.0046538257135816125,\n",
       " 4.8407452384216695e-05,\n",
       " 0.00044348539487479185,\n",
       " 0.0018232995622645344,\n",
       " 0.0015919051387513032,\n",
       " 0.00035502211160100652,\n",
       " 0.00095665599509377284,\n",
       " 7.4299464025397779e-05,\n",
       " 0.0016532489645091103,\n",
       " 5.3874812764798985e-05,\n",
       " 0.0020262148042217787,\n",
       " 0.0021995461271546385,\n",
       " 0.0022077425931123292,\n",
       " 4.9795732413295133e-05,\n",
       " 0.00056059897487071848,\n",
       " 0.00045697283720147865,\n",
       " 0.0013628470433815873,\n",
       " 0.0012546733011412283,\n",
       " 0.0005472922777549756,\n",
       " 4.8656475530234862e-05,\n",
       " 0.0015996213418048876,\n",
       " 0.00085075388152552683,\n",
       " 0.00036310564536751308,\n",
       " 0.00085861158086967106,\n",
       " 6.8347949449752693e-05,\n",
       " 5.6468128976539317e-05,\n",
       " 0.0087388957888597952,\n",
       " 0.00067449019785730002,\n",
       " 0.00024862907786467282,\n",
       " 0.0018722428063486962,\n",
       " 0.0013517832542116973,\n",
       " 0.0019861608185861002,\n",
       " 0.0022696960407950929,\n",
       " 0.0036024454887594397,\n",
       " 0.0065124962810020042,\n",
       " 0.0022383396010427791,\n",
       " 0.0048497323050923245,\n",
       " 0.00023760608859136785,\n",
       " 0.0021897658638139307,\n",
       " 0.0017116611231548003,\n",
       " 0.00166210792079187,\n",
       " 0.0020511763843151657,\n",
       " 7.2485593830224138e-05,\n",
       " 0.0022005334767114301,\n",
       " 0.00046205963973258528,\n",
       " 4.3669415957298212e-05,\n",
       " 0.00018883995029309047,\n",
       " 0.00068369520807069154,\n",
       " 0.016403571087439464,\n",
       " 0.0048083263152291873,\n",
       " 0.00094509415199573485,\n",
       " 0.00011597682645554811,\n",
       " 0.0018402845555260134,\n",
       " 0.0044099202806728837,\n",
       " 0.00037908932138648718,\n",
       " 0.00022995521437340011,\n",
       " 0.0036848861451463976,\n",
       " 7.1915354963066243e-05,\n",
       " 0.0010986098895774568,\n",
       " 0.00028978316061607833,\n",
       " 0.0054212420087614965,\n",
       " 0.0044215416019056262,\n",
       " 0.00068399387012751641,\n",
       " 0.00038682118745416588,\n",
       " 0.001895143799821861,\n",
       " 0.0023039800306819029,\n",
       " 0.0010881787199009862,\n",
       " 0.001078821973691888,\n",
       " 0.0026804498917834723,\n",
       " 0.00020682356281289993,\n",
       " 0.00046363441457170865,\n",
       " 0.00010041667771864602,\n",
       " 5.1039000318504097e-05,\n",
       " 0.0027689763338601498,\n",
       " 0.00036103637634641347,\n",
       " 9.919541132162421e-05,\n",
       " 0.0054212070195729318,\n",
       " 0.0061720309506064584,\n",
       " 0.0027471774883280212,\n",
       " 0.0020154617113461353,\n",
       " 0.00043804091907040984,\n",
       " 0.00046329651113243675,\n",
       " 0.0040049202635903331,\n",
       " 0.00076045225797419648,\n",
       " 0.00016456041938562287,\n",
       " 0.010266386487466353,\n",
       " 0.00053430590238605987,\n",
       " 0.00075412459462832434,\n",
       " 0.0019685278235943189,\n",
       " 0.0020197829019158757,\n",
       " 0.00016075241044002415,\n",
       " 0.039265425422607886,\n",
       " 0.0020363879883793204,\n",
       " 0.0021832761367321356,\n",
       " 6.54782031969524e-05,\n",
       " 7.734593207713978e-05,\n",
       " 0.00048944037215778533,\n",
       " 0.00013480617284650182,\n",
       " 0.0035388059867573888,\n",
       " 0.001060646556966441,\n",
       " 0.00012482166178183479,\n",
       " 0.00064739711683687434,\n",
       " 0.002041814782830045,\n",
       " 2.6302633470245064e-05,\n",
       " 0.00094053357618484616,\n",
       " 0.0027677363337575344,\n",
       " 0.00028802516366771184,\n",
       " 0.00051301385041786405,\n",
       " 0.00028153942960555103,\n",
       " 0.0037144326829681245,\n",
       " 0.00067680930279525133,\n",
       " 0.00015496839866843256,\n",
       " 0.00095123599191233063,\n",
       " 3.1286444373718634e-05,\n",
       " 0.0007782144299560669,\n",
       " 0.00099199617337809355,\n",
       " 0.0027432245041930474,\n",
       " 8.1979006016679406e-05,\n",
       " 0.0015266958433649196,\n",
       " 0.00040365207432112098,\n",
       " 0.00034003619500108217,\n",
       " 0.0022721665973138216,\n",
       " 0.00055954872130604245,\n",
       " 0.00012956008950312179,\n",
       " 0.00095745928211793429,\n",
       " 0.00017884656509012999,\n",
       " 0.0030926963779667653,\n",
       " 0.00013711991167830346,\n",
       " 0.00021303397775718317,\n",
       " 0.0016714504588487878,\n",
       " 4.3692941744452557e-05,\n",
       " 0.00013913472863607599,\n",
       " 0.0027387213149542315,\n",
       " 0.00090667102834519984,\n",
       " 0.00082227772895519195,\n",
       " 0.0063633506252701155,\n",
       " 6.4894458980211455e-05,\n",
       " 0.0067750951891121184,\n",
       " 0.00050958269315048619,\n",
       " 4.0166775078031184e-05,\n",
       " 0.005394671760956227,\n",
       " 5.2596604866977602e-06,\n",
       " 0.0012509980118214658,\n",
       " 0.00011343610362898647,\n",
       " 0.0012857950096908065,\n",
       " 0.00053668104599290101,\n",
       " 0.00094616966751800546,\n",
       " 0.0008264465561901291,\n",
       " 0.0020642012518212486,\n",
       " 0.00021277589120891546,\n",
       " 0.00012312328909847248,\n",
       " 0.0011670251344128297,\n",
       " 0.00087847709761079728,\n",
       " 0.00080386006175806285,\n",
       " 2.3807384905598809e-05,\n",
       " 0.0038740314132918092,\n",
       " 4.0493841596435628e-05,\n",
       " 0.0025646591146125716,\n",
       " 6.1388789355488595e-05,\n",
       " 0.0027712541445342439,\n",
       " 6.9889877416333865e-06,\n",
       " 0.0017355248295342917,\n",
       " 0.0019488196763299884,\n",
       " 0.0024437869755279409,\n",
       " 0.00011185739807885583,\n",
       " 7.1014848378186233e-05,\n",
       " 0.00082932021984074514,\n",
       " 0.0015323507812036049,\n",
       " 0.0012415934785798923,\n",
       " 0.0014379310151336061,\n",
       " 0.0014749201712865429,\n",
       " 0.0014035775222988871,\n",
       " 9.0852186622068361e-06,\n",
       " 0.00022080212019716925,\n",
       " 0.0015280391681582373,\n",
       " 0.00023910672514830173,\n",
       " 5.0121052845769674e-05,\n",
       " 9.8281652250303196e-05,\n",
       " 0.0021522740866544354,\n",
       " 0.0015007457365504265,\n",
       " 0.0082338626060328608,\n",
       " 0.0003126871046053183,\n",
       " 6.0547667528355496e-05,\n",
       " 0.00033562261113657439,\n",
       " 0.0024938314322028306,\n",
       " 0.00010317356216686466,\n",
       " 0.0011681163179195584,\n",
       " 0.00039961502471110315,\n",
       " 0.0046869277956577918,\n",
       " 0.00095601581857995897,\n",
       " 0.00012630428593720107,\n",
       " 0.00013103099452993307,\n",
       " 2.5771258436884006e-05,\n",
       " 4.965596519299844e-05,\n",
       " 0.0095756026486830022,\n",
       " 0.00028048742158671861,\n",
       " 0.0039307171879469169,\n",
       " 0.0002996156546073735,\n",
       " 0.00049523653454492896,\n",
       " 0.0013918027589917398,\n",
       " 0.00089189606634358675,\n",
       " 0.0045467653285685599,\n",
       " 5.5479092670633708e-05,\n",
       " 0.00017952226071818862,\n",
       " 0.00046495766879239115,\n",
       " 0.0022007033294706288,\n",
       " 0.00015943065463170907,\n",
       " 3.6536811565327945e-05,\n",
       " 0.0022662779034345786,\n",
       " 0.0017681419906352025,\n",
       " 0.0032646900849861694,\n",
       " 0.0021614297587035204,\n",
       " 0.002567614345586635,\n",
       " 0.000685744625432474,\n",
       " 0.0063448287632454871,\n",
       " 0.00061232555279539869,\n",
       " 0.0005685970499564899,\n",
       " 0.00042800106077268556,\n",
       " 0.00051920955694322892,\n",
       " 0.001146215583879369,\n",
       " 0.0039085587657085924,\n",
       " 0.0018370024527175968,\n",
       " 6.5854835580534475e-05,\n",
       " 0.00047717725175344553,\n",
       " 0.0023026229409007079,\n",
       " 2.530537831726378e-05,\n",
       " 0.00055078075365791001,\n",
       " 0.00069381473080706467,\n",
       " 0.0066806708679226917,\n",
       " 2.369542995335241e-05,\n",
       " 0.00094078914588087838,\n",
       " 0.00054197098333414351,\n",
       " 0.0039876228117136005,\n",
       " 0.00026408000643323787,\n",
       " 0.0012503268742265917,\n",
       " 0.0028630022890782222,\n",
       " 0.0024298677179714792,\n",
       " 0.00053846890899038606,\n",
       " 0.00022481146839274999,\n",
       " 0.0015846825135845577,\n",
       " 5.5378603054753619e-05,\n",
       " 0.0037807892956386742,\n",
       " 0.001342241954738035,\n",
       " 0.0033784694856197903,\n",
       " 0.0036313712342384416,\n",
       " 0.0046591901738529274,\n",
       " 0.00069613272420396342,\n",
       " 0.00069483399315120001,\n",
       " 0.00010702414057805316,\n",
       " 0.0045273737239626115,\n",
       " 0.00052315130026471296,\n",
       " 0.0012759809725666205,\n",
       " 1.1351748021524582e-05,\n",
       " 0.00094861007193918455,\n",
       " 0.009526127371781646,\n",
       " 0.00013192747924169177,\n",
       " 0.0054240277550230117,\n",
       " 0.0037765004567254581,\n",
       " 0.00066766918572508816,\n",
       " 0.00035852030595090879,\n",
       " 0.00046582813284363523,\n",
       " 0.0017074593044234798,\n",
       " 3.4847356171164007e-05,\n",
       " 2.1224167385410893e-05,\n",
       " 0.00032933757531143111,\n",
       " 0.00022575470065855602,\n",
       " 0.00010389527718357387,\n",
       " 0.0031410720682134379,\n",
       " 0.0026338217694746951,\n",
       " 7.3835398139946074e-05,\n",
       " 9.4568548240852503e-05,\n",
       " 1.6258728366592855e-05,\n",
       " 0.00031424306041842132,\n",
       " 0.0016197052348039978,\n",
       " 0.00044340089985648912,\n",
       " 0.0037979343302026515,\n",
       " 0.0013688529140084796,\n",
       " 0.0010635343204887242,\n",
       " 0.010329539970576795,\n",
       " 2.8388145864131351e-05,\n",
       " 0.0056320467291479572,\n",
       " 0.00078181288912603528,\n",
       " 0.0010319788066897252,\n",
       " 0.00050197647605135839,\n",
       " 9.8103417858050061e-05,\n",
       " 0.0019118248369767252,\n",
       " 0.0035464885492631762,\n",
       " 0.00040321113177987212,\n",
       " 0.0027114613919691898,\n",
       " 0.00026667507636872863,\n",
       " 0.00046244676575101396,\n",
       " 0.0024833434319667274,\n",
       " 0.00012500482458325149,\n",
       " 0.00026403167068516715,\n",
       " 0.00013360718408347329,\n",
       " 0.00059740440324847362,\n",
       " 0.0027764190424269487,\n",
       " 4.1314214886944045e-05,\n",
       " 0.00034265966458900149,\n",
       " 0.00030598370279195171,\n",
       " 0.0018291203550695741,\n",
       " 0.00059423654858927475,\n",
       " 0.0019319749454368613,\n",
       " 0.00073294042318216494,\n",
       " 0.0078518893224393121,\n",
       " 0.009983375437663802,\n",
       " 0.00067143659605448081,\n",
       " 0.0045327760386146936,\n",
       " 0.00084331835393722124,\n",
       " 0.00022795412572942467,\n",
       " 0.0027930524691485415,\n",
       " 0.00014069328290906976,\n",
       " 0.00012639818703461507,\n",
       " 0.00013710515352342566,\n",
       " 2.8942090350889952e-05,\n",
       " 3.7853943097899686e-05,\n",
       " 0.0049348600832878784,\n",
       " 3.5582680378584763e-05,\n",
       " 0.00093331766302789715,\n",
       " 0.00012015782514762283,\n",
       " 0.00093184528105925339,\n",
       " 1.5085017312569468e-05,\n",
       " 2.3546133079966468e-05,\n",
       " 0.0006547562853689863,\n",
       " 0.0017818918286293408,\n",
       " 0.00027152871037338603,\n",
       " 0.0082694439321514372,\n",
       " 0.00062523947505311651,\n",
       " 0.0002635818459652348,\n",
       " 0.0037878227040591267,\n",
       " 0.00109959229775538,\n",
       " 5.4265742114087274e-05,\n",
       " 0.0008328850292705118,\n",
       " 0.0031379597879471902,\n",
       " 0.0046985574804246522,\n",
       " ...]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習曲線のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11f455898>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUREAPj/QtMIsHrULJIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "l = list(range(3))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2890506071192074"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 400)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.264747083589044"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# クロスエントロピー誤差\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayer3_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://deepage.net/features/numpy-numpy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バックプロパゲーション_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # def backprob(mini_X_train, mini_y_train, rayer3_out , w1, w2,w3,b1, b2, b3):\n",
    "# delta1 = rayer3_out - mini_y_train\n",
    "# delta2 = (1 - np.tanh(rayer2_pre)**2)*(delta1.dot(w3.T))\n",
    "# delta3 = (1 - np.tanh(rayer1_pre)**2)*(delta2.dot(w2.T))\n",
    "\n",
    "# b3_grad = delta1\n",
    "# w3_grad = rayer2_out.T.dot(delta1) \n",
    "\n",
    "# b2_grad = delta2\n",
    "# w2_grad = rayer1_out.T.dot(delta2) \n",
    "\n",
    "# b1_grad = delta3\n",
    "# w1_grad = mini_X_train.T.dot(delta3) \n",
    "#     # return delta2, delta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 400)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00091635,  0.00243333,  0.0017991 , ..., -0.00125521,\n",
       "        -0.00160988, -0.00139451],\n",
       "       [-0.00093514,  0.00244135,  0.00176375, ..., -0.00125496,\n",
       "        -0.00160646, -0.00139063],\n",
       "       [-0.0009062 ,  0.00242778,  0.00178479, ..., -0.00122928,\n",
       "        -0.00152835, -0.00138713],\n",
       "       ..., \n",
       "       [ 0.00284563, -0.00149056,  0.0012257 , ..., -0.0012071 ,\n",
       "        -0.00036211,  0.00144608],\n",
       "       [-0.00122655, -0.00282252,  0.00042313, ...,  0.00081647,\n",
       "        -0.00045074,  0.00307157],\n",
       "       [-0.00123783, -0.00276446,  0.00041889, ...,  0.00079577,\n",
       "        -0.00044746,  0.00307833]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 400)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 400)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def gradient_descent_minibatch(X,y,alpha,epoch,lam,batch_size):\n",
    "#     cost=[]\n",
    "#     thetas=[]\n",
    "#     np.random.seed(0)\n",
    "#     w1=np.random.rand()\n",
    "#     w2=np.random.rand()\n",
    "#     b1=np.random.rand()\n",
    "#     b2=np.random.rand()\n",
    "#     epoch = 1\n",
    "#     for i in range(epoch):\n",
    "         \n",
    "#             # Xからランダムにbatch_size分とりだす\n",
    "#         train_size = X.shape[0]\n",
    "#         for mini_X_train,mini_y_train in batch_size:\n",
    "# #             a2=forward(x_batch, w1, w2, b1, b2)\n",
    "# #             costs.append(cross_entropy_error(a2, y_batch, w1, w2, lam))\n",
    "# #             delta2, delta3=backprob(x_batch, y_batch, w1, w2, b1, b2)\n",
    "\n",
    "#             w1= w1 - alpha*w1_grad\n",
    "#             w2= w2 - alpha*w2_grad\n",
    "#             w3= w3 - alpha*w3_grad\n",
    "#             b1 = b1 - alpha*b1_grad.sum(axis=0)\n",
    "#             b2 = b2 - alpha*b2_grad.sum(axis=0)\n",
    "#             b3 = b3 - alpha*b3_grad.sum(axis=0)\n",
    "\n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-695b46462d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradient_descent_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "gradient_descent_minibatch(X,y,alpha,epoch,lam,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-63feb3ae1029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cost' is not defined"
     ]
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの仕組み\n",
    "多くの層を持つニューラルネットワークを用いた学習を、ディープラーニングという。ディープラーニングは、ヒトの脳に部分的に迫る高度な学習を行うことができるのが特徴。複数の層からなるネットワークに入力と出力があるが、このようなネットワークの各パラメータを最適化することでネットワーク自体が学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バックプロパゲーションとは何か\n",
    "バックプロパゲーションとは、ニューラルネットワークを学習させる際に用いられるアルゴリズムで、出力と正解の誤差がネットワークを逆伝搬することにより、ネットワークの重みとバイアスを最適化する。     \n",
    "バックプロパゲーションの手順としては、順伝搬により得られた出力と、あらかじめ用意された正解の誤差を、層を1つずつ遡るように逆伝搬させる。その際に、伝搬した誤差をもとに各層で重みとバイアスの更新量を求める。そして、すべての各層の重みとバイアスを少しずつ更新する。これを繰り返すことで、次第にネットワークは誤差が最小になるように最適化されていき、学習が進行する。学習が成功したネットワークは、任意の入力に対して柔軟に認識・判断ができるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数の役割とは何か\n",
    "活性化関数とは、言わばニューロンを興奮させるための関数である。ニューロンへの入力と重みを掛けたものの総和にバイアスを足し合わせて統合された値を、ニューロンの興奮状態を表す信号に変換する。活性化関数がないとニューロンの演算は単なる積の総和になってしまい、ニューラルネットワークから複雑な表現をする力が失われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク分類器のクラスを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class ScratchSimpleNeuralNetrowkClassifier():\n",
    "#     \"\"\"\n",
    "#     シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "\n",
    "#     Attributes\n",
    "#     ----------\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, verbose = True):\n",
    "#         self.verbose = verbose\n",
    "#         pass\n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         \"\"\"\n",
    "#         ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#             学習用データの特徴量\n",
    "#         y : 次の形のndarray, shape (n_samples, )\n",
    "#             学習用データの正解値\n",
    "#         X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "#             検証用データの特徴量\n",
    "#         y_val : 次の形のndarray, shape (n_samples, )\n",
    "#             検証用データの正解値\n",
    "#         \"\"\"\n",
    "\n",
    "#         if self.verbose:\n",
    "#             #verboseをTrueにした際は学習過程などを出力する\n",
    "#             print()\n",
    "#         pass\n",
    "\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#             サンプル\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#             次の形のndarray, shape (n_samples, 1)\n",
    "#             推定結果\n",
    "#         \"\"\"\n",
    "\n",
    "#         pass\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # import sys, os\n",
    "# # import pickle\n",
    "# # sys.path.append(os.pardir)\n",
    "\n",
    "# # 活性化関数\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def softmax(x):\n",
    "#     x = x - np.max(x)\n",
    "#     return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# def cross_entropy(y, t):\n",
    "#     return (-1) * np.sum(t * np.log(y))\n",
    "\n",
    "# # 1層のみの計算\n",
    "# def eval(X_train, W, B, h):\n",
    "#     A = np.dot(X_train, W1) + B\n",
    "#     Z = h(A)\n",
    "#     return Z\n",
    "\n",
    "# for i in range(len(X_train)):\n",
    "#         h = eval(X_train[i], sigmoid, softmax)\n",
    "#         p = np.argmax(y)\n",
    "#         if p == y[i]:\n",
    "#             ok_cnt += 1\n",
    "# #     print(\"data_num:\" + str(len(x)))\n",
    "# #     print(\"ok_count:\" + str(ok_cnt))\n",
    "# #     print(\"Accuracy:\" + str(float(ok_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def softmax(x):\n",
    "#     if x.ndim == 2:\n",
    "#         x = x.T\n",
    "#         x = x - np.max(x, axis=0)\n",
    "#         y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "#         return y.T\n",
    "\n",
    "#     x = x - np.max(x)\n",
    "#     return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# np.sum(softmax(X),axis=1) #チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# import pickle\n",
    "# sys.path.append(os.pardir)\n",
    "\n",
    "# # 活性化関数\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def softmax(x):\n",
    "#     c = np.max(x)\n",
    "#     top = np.exp(x - c)\n",
    "#     bottom = np.sum(np.exp(x - c))\n",
    "#     return top / bottom\n",
    "\n",
    "# # 3層の順方向伝播\n",
    "# def eval_3_network(X, network, h, o):\n",
    "#     W1, W2, W3 \n",
    "#     b1, b2, b3 \n",
    "#     Z1 = eval_single_network(X, W1, B1, h)\n",
    "#     Z2 = eval_single_network(Z1, W2, B2, h)\n",
    "#     Z3 = eval_single_network(Z2, W3, B3, o)\n",
    "#     return Z3\n",
    "\n",
    "# # 1層のみの計算\n",
    "# def eval(X, W, B, h):\n",
    "#     A = np.dot(X, W1) + B\n",
    "#     Z = h(A)\n",
    "#     return Z\n",
    "\n",
    "# # 学習済み重みを読み込む\n",
    "# def load_trained_network():\n",
    "#     with open('sample_weight.pkl', 'rb') as f:\n",
    "#         network = pickle.load(f)\n",
    "#     return network\n",
    "\n",
    "# # テストデータを読み込む\n",
    "# def load_test_data():\n",
    "#     (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "#     return x_test, t_test\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     network = load_trained_network()\n",
    "#     x, t = load_test_data()\n",
    "#     ok_cnt = 0\n",
    "#     for i in range(len(x)):\n",
    "#         y = eval_3_network(x[i], network, sigmoid, softmax)\n",
    "#         p = np.argmax(y)\n",
    "#         if p == t[i]:\n",
    "#             ok_cnt += 1\n",
    "#     print(\"data_num:\" + str(len(x)))\n",
    "#     print(\"ok_count:\" + str(ok_cnt))\n",
    "#     print(\"Accuracy:\" + str(float(ok_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A1 = np.dot(X_train, W1) + B1\n",
    "\n",
    "# Z1 = sigmoid(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# シグモイド関数の実装\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ソフトマックス関数の実装\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差の実装\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = 10\n",
    "    return -np.sum(t * np.log(y + le-7)) / batch.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （オプション）誤分類の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "語分類結果を並べて表示する。画像の上の表示は「推定結果/正解」である。\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "y_pred : 推定値のndarray (n_samples,)\n",
    "y_val : 検証用データの正解ラベル(n_samples,)\n",
    "X_val : 検証用データの特徴量（n_samples, n_features)\n",
    "\"\"\"\n",
    "\n",
    "num = 36 # いくつ表示するか\n",
    "\n",
    "true_false = y_pred==y_val\n",
    "false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "if false_list.shape[0] < num:\n",
    "    num = false_list.shape[0]\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "for i in range(num):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
