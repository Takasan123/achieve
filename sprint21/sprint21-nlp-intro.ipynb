{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIF_sprint21_nlp_intro\n",
    "1.この課題の目的  \n",
    "自然言語の機械学習での扱われ方を学ぶ   \n",
    "自然言語のベクトル化を学ぶ   \n",
    "\n",
    "【目的としないこと】   \n",
    "分類など具体的なタスクの扱い方  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['挫折を', '折を経', 'を経験', '経験し', '験した', 'した事', 'た事が', '事がな', 'がない', 'ない者', 'い者は', '者は、', 'は、何', '、何も', '何も新', 'も新し', '新しい', 'しい事', 'い事に', '事に挑', 'に挑戦', '挑戦し', '戦した', 'したこ', 'たこと', 'ことが', 'とが無', 'が無い', '無いと', 'いとい', 'という', 'いうこ', 'うこと', 'ことだ', 'とだ。']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def ngram(nlp,hoge):\n",
    "    box = []\n",
    "    for i in range(0,len(hoge)-nlp+1):\n",
    "        box.append(hoge[i:nlp+i])\n",
    "    return box\n",
    "\n",
    "# アインシュタインの言葉\n",
    "hoge = \"挫折を経験した事がない者は、何も新しい事に挑戦したことが無いということだ。\"\n",
    "words_hoge = hoge.split(\" \")\n",
    "\n",
    "#バイグラム\n",
    "box = ngram(3,hoge)\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84194704 -0.49907325 -1.54317787]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 入力\n",
    "w = np.random.randn(7, 3)    # 重み\n",
    "h = np.dot(c, w)   # 中間ノード\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 分散表現の作成\n",
    "### textファイルを作成\n",
    "English is spoken by many people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ターミナル上で、以下のコマンドに書き換えて、textファイルを学習（CBOWモデルを使う）\n",
    "\n",
    "./word2vec -train text.txt -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -text 1 -iter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectors.binをvectors.txtに替え中身を見てみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1 200\n",
    "</s> 0.002001 0.002210 -0.001915 -0.001639 0.000683 0.001511 0.000470 0.000106 -0.001802 0.001109 -0.002178 0.000625 -0.000376 -0.000479 -0.001658 -0.000941 0.001290 0.001513 0.001485 0.000799 0.000772 -0.001901 -0.002048 0.002485 0.001901 0.001545 -0.000302 0.002008 -0.000247 0.000367 -0.000075 -0.001492 0.000656 -0.000669 -0.001913 0.002377 0.002190 -0.000548 -0.000113 0.000255 -0.001819 -0.002004 0.002277 0.000032 -0.001291 -0.001521 -0.001538 0.000848 0.000101 0.000666 -0.002107 -0.001904 -0.000065 0.000572 0.001275 -0.001585 0.002040 0.000463 0.000560 -0.000304 0.001493 -0.001144 -0.001049 0.001079 -0.000377 0.000515 0.000902 -0.002044 -0.000992 0.001457 0.002116 0.001966 -0.001523 -0.001054 -0.000455 0.001001 -0.001894 0.001499 0.001394 -0.000799 -0.000776 -0.001119 0.002114 0.001956 -0.000590 0.002107 0.002410 0.000908 0.002491 -0.001556 -0.000766 -0.001054 -0.001454 0.001407 0.000790 0.000212 -0.001097 0.000762 0.001530 0.000097 0.001140 -0.002476 0.002157 0.000240 -0.000916 -0.001042 -0.000374 -0.001468 -0.002185 -0.001419 0.002139 -0.000885 -0.001340 0.001159 -0.000852 0.002378 -0.000802 -0.002294 0.001358 -0.000037 -0.001744 0.000488 0.000721 -0.000241 0.000912 -0.001979 0.000441 0.000908 -0.001505 0.000071 -0.000030 -0.001200 -0.001416 -0.002347 0.000011 0.000076 0.000005 -0.001967 -0.002481 -0.002373 -0.002163 -0.000274 0.000696 0.000592 -0.001591 0.002499 -0.001006 -0.000637 -0.000702 0.002366 -0.001882 0.000581 -0.000668 0.001594 0.000020 0.002135 -0.001410 -0.001303 -0.002096 -0.001833 -0.001600 -0.001557 0.001222 -0.000933 0.001340 0.001845 0.000678 0.001475 0.001238 0.001170 -0.001775 -0.001717 -0.001828 -0.000066 0.002065 -0.001368 -0.001530 -0.002098 0.001653 -0.002089 -0.000290 0.001089 -0.002309 -0.002239 0.000721 0.001762 0.002132 0.001073 0.001581 -0.001564 -0.001820 0.001987 -0.001382 0.000877 0.000287 0.000895 -0.000591 0.000099 -0.000843 -0.000563 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### textファイルを学習（skip-gramモデルを使う）\n",
    "./word2vec -train text.txt -output vectors.bin -sg 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -text 1 -iter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectors.binをvectors.txtに替え中身を見てみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 200\n",
    "</s> 0.002001 0.002210 -0.001915 -0.001639 0.000683 0.001511 0.000470 0.000106 -0.001802 0.001109 -0.002178 0.000625 -0.000376 -0.000479 -0.001658 -0.000941 0.001290 0.001513 0.001485 0.000799 0.000772 -0.001901 -0.002048 0.002485 0.001901 0.001545 -0.000302 0.002008 -0.000247 0.000367 -0.000075 -0.001492 0.000656 -0.000669 -0.001913 0.002377 0.002190 -0.000548 -0.000113 0.000255 -0.001819 -0.002004 0.002277 0.000032 -0.001291 -0.001521 -0.001538 0.000848 0.000101 0.000666 -0.002107 -0.001904 -0.000065 0.000572 0.001275 -0.001585 0.002040 0.000463 0.000560 -0.000304 0.001493 -0.001144 -0.001049 0.001079 -0.000377 0.000515 0.000902 -0.002044 -0.000992 0.001457 0.002116 0.001966 -0.001523 -0.001054 -0.000455 0.001001 -0.001894 0.001499 0.001394 -0.000799 -0.000776 -0.001119 0.002114 0.001956 -0.000590 0.002107 0.002410 0.000908 0.002491 -0.001556 -0.000766 -0.001054 -0.001454 0.001407 0.000790 0.000212 -0.001097 0.000762 0.001530 0.000097 0.001140 -0.002476 0.002157 0.000240 -0.000916 -0.001042 -0.000374 -0.001468 -0.002185 -0.001419 0.002139 -0.000885 -0.001340 0.001159 -0.000852 0.002378 -0.000802 -0.002294 0.001358 -0.000037 -0.001744 0.000488 0.000721 -0.000241 0.000912 -0.001979 0.000441 0.000908 -0.001505 0.000071 -0.000030 -0.001200 -0.001416 -0.002347 0.000011 0.000076 0.000005 -0.001967 -0.002481 -0.002373 -0.002163 -0.000274 0.000696 0.000592 -0.001591 0.002499 -0.001006 -0.000637 -0.000702 0.002366 -0.001882 0.000581 -0.000668 0.001594 0.000020 0.002135 -0.001410 -0.001303 -0.002096 -0.001833 -0.001600 -0.001557 0.001222 -0.000933 0.001340 0.001845 0.000678 0.001475 0.001238 0.001170 -0.001775 -0.001717 -0.001828 -0.000066 0.002065 -0.001368 -0.001530 -0.002098 0.001653 -0.002089 -0.000290 0.001089 -0.002309 -0.002239 0.000721 0.001762 0.002132 0.001073 0.001581 -0.001564 -0.001820 0.001987 -0.001382 0.000877 0.000287 0.000895 -0.000591 0.000099 -0.000843 -0.000563 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然言語処理の分野ではどのような応用事例があるか\n",
    "・分かりやすい例では、検索エンジンや機械翻訳がある。その他には、質問応答システムやIME（かな漢字変換）、文章の自動要約や感情分析などがある。   \n",
    "・例えば、質問応答システムの代表例としては、IBMのWatsonが有名である。2011年のアメリカのクイズ番組において、Watsonは誰よりもクイズに正確に答え、歴代のチャンピオンに勝利し、世間から多くの注目を集めた。この時期あたりから、AIに関する世間の期待が高まってきたと見られる。Watsonは意志決定支援システムとしても利用されており、最近では、過去の膨大な医療データを活用して、難病患者の正しい治療法を提案し、患者の命を救った事例が報告されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### なぜ自然言語をベクトルの形にするのか\n",
    "最近では単語のベクトル表現を大規模なテキストから自動的に学習する手法が盛んに研究されている。文字の情報をベクトルに変換することで、単語の意味を考慮した「足し算」や「引き算」可能になる。また、単語同士の意味の近さを計算することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布仮説とは何か\n",
    "分布仮説とは、「単語の意味は、周囲の単語によって形成される」という仮説。これは、単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されるという意味で、単語をベクトルで表す自然言語処理の研究の多くが、この仮説に基づいている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vecにはskip-gramとCBOWの2つの方法があるがどのようなものか。どう選択すれば良いのか\n",
    "・CBOWはコンテキストが複数あり、その複数のコンテキストから中央の単語（ターゲット）を推測する。一方、skip-gramでは、中央の単語（ターゲット）から、周囲の複数ある単語（コンテキスト）を推測する。    \n",
    "・２つの方法のうち、単語の分散表現の精度が高いのは、skip-gramである。特に、コーパスが大規模になるにつれて、低頻出の単語や類推問題の性能の点において、skip-gramの方が優れた結果が得られる傾向にある。    \n",
    "・一方で、学習速度の点では、CBOWの方が高速である。これは、CBOWがひとつのターゲットの損失を求めるのに対して、skip-gramは、コンテキストの数だけ損失を求めるため、計算コストが大きくなるためである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウントベースと呼ばれる手法もあるがどういったものか。利用する上でどのような違いがあるか\n",
    "・カウントベースの手法とは、ある単語に注目した場合、その周囲にどのような単語がどれだけ現れるのかをカウントし、それを集計する手法である。一方で、word2vecに代表される推論ベースの手法がある。   \n",
    "・大規模なコーパスを扱う場合、カウントベースの手法は、コーパス全体の統計データを利用して、SVD（特異値分解；次元削減を行う方法）など１回の処理を使って、単語の分散表現を獲得する。一方で、推論ベースの手法では、ニューラルネットワークを用いる場合は、ミニバッチで学習するのが一般的である。また、ニューラルネットワークの学習は、複数のマシンや複数GPUの利用による並列計算も可能であり、全体の学習も高速化できる。したがって、学習速度の点では、推論ベースの方が優っている。    \n",
    "・また、推論ベースではパラメータの再学習が行えるため、単語の分散表現の更新作業が発生した場合でも、途中から作業ができる（カウントベースではゼロから計算を行う必要がある）。さらに、単語の分散表現によって、カウントベースでは単語の類似性が捉えられるが、推論ベースでは単語の類似性に加えて、さらに複雑な単語間のパターンも捉えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vecなどの手法に入力する前にどのような前処理があるか。英語の場合と日本語の場合を考える\n",
    "英語の場合、    \n",
    "・単語の分割    \n",
    "・単語の正規化（大文字小文字）   \n",
    "・Stemming処理（活用形の処理）    \n",
    "が必要\n",
    "\n",
    "日本語の場合はこの単語分割が難しく、形態素解析が必要となる。   \n",
    "\n",
    "その後に、   \n",
    "・テキストを単語IDに変換する   \n",
    "・テキストからコンテキストとターゲットを作る     \n",
    "・one-hot表現に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
